{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae134a62-a8dd-44e7-be13-3205875d102b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in c:\\users\\fayzt\\appdata\\roaming\\python\\python311\\site-packages (8.2.103)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.0 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from ultralytics) (1.24.3)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from ultralytics) (3.7.1)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in c:\\users\\fayzt\\appdata\\roaming\\python\\python311\\site-packages (from ultralytics) (4.10.0.84)\n",
      "Requirement already satisfied: pillow>=7.1.2 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from ultralytics) (9.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from ultralytics) (6.0)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from ultralytics) (2.31.0)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from ultralytics) (1.10.1)\n",
      "Requirement already satisfied: torch>=1.8.0 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from ultralytics) (2.0.1)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from ultralytics) (0.15.2)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from ultralytics) (4.65.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from ultralytics) (5.9.0)\n",
      "Requirement already satisfied: py-cpuinfo in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from ultralytics) (8.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from ultralytics) (1.5.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from ultralytics) (0.13.2)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in c:\\users\\fayzt\\appdata\\roaming\\python\\python311\\site-packages (from ultralytics) (2.0.8)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (23.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2022.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2024.6.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from tqdm>=4.64.0->ultralytics) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d799de1-5dbc-4f3c-9a54-2eb418d0fb1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\fayzt\\appdata\\roaming\\python\\python311\\site-packages (4.10.0.84)\n",
      "Requirement already satisfied: mediapipe in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (0.10.14)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from opencv-python) (1.24.3)\n",
      "Requirement already satisfied: absl-py in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from mediapipe) (2.1.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from mediapipe) (22.1.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from mediapipe) (24.3.25)\n",
      "Requirement already satisfied: jax in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from mediapipe) (0.4.31)\n",
      "Requirement already satisfied: jaxlib in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from mediapipe) (0.4.31)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from mediapipe) (3.7.1)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from mediapipe) (4.10.0.84)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from mediapipe) (4.25.4)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from mediapipe) (0.5.0)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe) (1.15.1)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from jax->mediapipe) (0.4.0)\n",
      "Requirement already satisfied: opt-einsum in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from jax->mediapipe) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.10 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from jax->mediapipe) (1.10.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (23.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (2.8.2)\n",
      "Requirement already satisfied: pycparser in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecbf36a0-503e-4650-99ab-c1d461e554b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (2.17.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.17.0 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from tensorflow) (2.17.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (23.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.25.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.65.4)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.17.1)\n",
      "Requirement already satisfied: keras>=3.2.0 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.5.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.24.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: rich in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2024.6.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\fayzt\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff1152b8-f8cd-460b-8711-b3973b853da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import tkinter as tk\n",
    "from tkinter import simpledialog\n",
    "import threading\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from ultralytics import YOLO  # Import YOLOv8\n",
    "import warnings\n",
    "\n",
    "# Suppress TensorFlow warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"tensorflow\")\n",
    "\n",
    "# Initialize Mediapipe and YOLO models\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "yolo_model = YOLO('yolov8n.pt')\n",
    "\n",
    "# Load pre-trained CNN model\n",
    "cnn_model = tf.keras.models.load_model(\"focused_distracted_model.h5\")\n",
    "\n",
    "# Global variables\n",
    "class_duration = 0\n",
    "distraction_time = 0\n",
    "face_detected = False\n",
    "distraction_start = None\n",
    "interval = 0\n",
    "distraction_threshold = 0\n",
    "class_end_time = 0\n",
    "pop_up_response = None\n",
    "yes_count = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f014434b-dd0f-4f55-8da5-e45e63b2a664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_duration():\n",
    "    global class_duration, interval, distraction_threshold, class_end_time\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()  \n",
    "    class_duration = int(simpledialog.askinteger(\"Input\", \"Enter class duration in minutes:\"))\n",
    "    if class_duration:\n",
    "        class_duration *= 60  \n",
    "        interval = class_duration / 4\n",
    "        distraction_threshold = class_duration * 0.25\n",
    "        class_end_time = time.time() + class_duration\n",
    "        start_monitoring()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d80a93ff-5c4b-4cdf-8738-90163d3de059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question():\n",
    "    global pop_up_response, yes_count, distraction_start, distraction_time\n",
    "\n",
    "    def on_yes():\n",
    "        global pop_up_response, yes_count, distraction_start, distraction_time\n",
    "        pop_up_response = \"Yes\"\n",
    "        yes_count += 1\n",
    "       \n",
    "        if distraction_start is not None:\n",
    "            distraction_time += time.time() - distraction_start\n",
    "            distraction_start = None\n",
    "        root.destroy()\n",
    "\n",
    "    def check_timeout():\n",
    "        global pop_up_response, distraction_start\n",
    "        if pop_up_response is None:\n",
    "           \n",
    "            if distraction_start is None:\n",
    "                distraction_start = time.time()\n",
    "\n",
    "    # Create a pop-up window\n",
    "    root = tk.Tk()\n",
    "    root.title(\"Student Monitoring\")\n",
    "    root.geometry(\"300x150\")\n",
    "    root.eval('tk::PlaceWindow . center')  \n",
    "\n",
    "    tk.Label(root, text=\"Are you here?\", font=(\"Arial\", 14)).pack(pady=20)\n",
    "    tk.Button(root, text=\"Yes\", command=on_yes, width=10).pack(pady=20)\n",
    "\n",
    "   \n",
    "    root.after(15000, check_timeout)\n",
    "\n",
    "    \n",
    "    root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10f4517a-78b9-4c11-b51c-023655725927",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multitasking_check():\n",
    "    while time.time() < class_end_time:\n",
    "        time.sleep(interval)  \n",
    "        ask_question()  \n",
    "        if pop_up_response == \"No\":\n",
    "            print(\"Student did not respond in time!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7827cf7d-0148-48da-a4ea-d5d084ab0079",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_monitoring():\n",
    "    global distraction_time, face_detected, distraction_start\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    face_mesh = mp_face_mesh.FaceMesh()\n",
    "\n",
    "    # Start the multitasking check in a separate thread\n",
    "    threading.Thread(target=multitasking_check, daemon=True).start()\n",
    "\n",
    "    while cap.isOpened():\n",
    "        current_time = time.time()\n",
    "        if current_time >= class_end_time:  # Check if class duration has ended\n",
    "            break\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # YOLOv8: Check if a person is in the frame\n",
    "        yolo_results = yolo_model(frame)\n",
    "        person_detected = False\n",
    "        for result in yolo_results:\n",
    "            boxes = result.boxes\n",
    "            for box in boxes:\n",
    "                cls = int(box.cls[0])\n",
    "                label = result.names[cls]\n",
    "                if label == 'person':  # Check if YOLOv8 detects a person\n",
    "                    person_detected = True\n",
    "                    break\n",
    "\n",
    "        if not person_detected:\n",
    "            if distraction_start is None:\n",
    "                distraction_start = time.time()\n",
    "            distraction_time += time.time() - distraction_start\n",
    "            distraction_start = time.time()\n",
    "            print(\"No person detected - Adding to distraction time\")\n",
    "            continue\n",
    "\n",
    "        # Mediapipe for face detection and CNN for classification\n",
    "        process_frame_with_cnn(frame, face_mesh)\n",
    "\n",
    "        # Display the frame\n",
    "        cv2.imshow(\"Student Monitoring\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    show_result()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2d7a601-2753-4f43-87b4-9dd1ddf6264a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame_with_cnn(frame, face_mesh):\n",
    "    global distraction_time, distraction_start, face_detected\n",
    "\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(image)\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        face_detected = True\n",
    "        if distraction_start:\n",
    "            distraction_time += time.time() - distraction_start\n",
    "            distraction_start = None\n",
    "\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            mp_drawing.draw_landmarks(frame, face_landmarks, mp_face_mesh.FACEMESH_CONTOURS)\n",
    "\n",
    "        # Preprocess the face for CNN\n",
    "        face_resized = cv2.resize(frame, (224, 224))\n",
    "        face_normalized = face_resized / 255.0\n",
    "        face_input = np.expand_dims(face_normalized, axis=0)\n",
    "\n",
    "        # Predict focus/distracted\n",
    "        confidence = cnn_model.predict(face_input)[0][0]\n",
    "        if confidence > 0.5:\n",
    "            print(f\"Prediction: Distracted (Confidence: {confidence:.2f})\")\n",
    "            if distraction_start is None:\n",
    "                distraction_start = time.time()\n",
    "        else:\n",
    "            print(f\"Prediction: Focused (Confidence: {confidence:.2f})\")\n",
    "            if distraction_start:\n",
    "                distraction_time += time.time() - distraction_start\n",
    "                distraction_start = None\n",
    "    else:\n",
    "        if not face_detected and distraction_start is None:\n",
    "            distraction_start = time.time()\n",
    "        if distraction_start:\n",
    "            distraction_time += time.time() - distraction_start\n",
    "            distraction_start = time.time()\n",
    "        face_detected = False\n",
    "        print(\"No face detected - Student is distracted\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0f18c1a-e7bf-40a0-8705-f245b2201b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_result():\n",
    "    global distraction_time, distraction_threshold, yes_count\n",
    "    distraction_minutes = distraction_time / 60  \n",
    "    status = \"Present\" if distraction_time < distraction_threshold else \"Absent\"\n",
    "\n",
    "    root = tk.Tk()\n",
    "    root.title(\"Monitoring Result\")\n",
    "    root.geometry(\"400x200\")\n",
    "    root.eval('tk::PlaceWindow . center')\n",
    "\n",
    "    tk.Label(root, text=\"Monitoring Complete\", font=(\"Arial\", 16)).pack(pady=10)\n",
    "    tk.Label(root, text=f\"Student Status: {status}\", font=(\"Arial\", 14)).pack(pady=5)\n",
    "    tk.Label(root, text=f\"Distraction Time: {distraction_minutes:.2f} minutes\", font=(\"Arial\", 14)).pack(pady=5)\n",
    "    tk.Label(root, text=f\"Questions Answered 'Yes': {yes_count}\", font=(\"Arial\", 14)).pack(pady=5)\n",
    "\n",
    "    tk.Button(root, text=\"Close\", command=root.destroy, width=10).pack(pady=20)\n",
    "    root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02a3ad8-85bd-42ec-bcd4-92424f14e788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 181.5ms\n",
      "Speed: 3.0ms preprocess, 181.5ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fayzt\\anaconda3\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 166.0ms\n",
      "Speed: 5.0ms preprocess, 166.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 161.2ms\n",
      "Speed: 3.0ms preprocess, 161.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 156.5ms\n",
      "Speed: 4.0ms preprocess, 156.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 152.5ms\n",
      "Speed: 2.0ms preprocess, 152.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 153.9ms\n",
      "Speed: 3.5ms preprocess, 153.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 147.5ms\n",
      "Speed: 3.0ms preprocess, 147.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 144.4ms\n",
      "Speed: 3.5ms preprocess, 144.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 145.0ms\n",
      "Speed: 3.0ms preprocess, 145.0ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Focused (Confidence: 0.01)\n",
      "\n",
      "0: 480x640 1 person, 144.0ms\n",
      "Speed: 2.5ms preprocess, 144.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Focused (Confidence: 0.01)\n",
      "\n",
      "0: 480x640 1 person, 147.4ms\n",
      "Speed: 2.0ms preprocess, 147.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Prediction: Focused (Confidence: 0.01)\n",
      "\n",
      "0: 480x640 1 person, 150.1ms\n",
      "Speed: 2.0ms preprocess, 150.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Focused (Confidence: 0.02)\n",
      "\n",
      "0: 480x640 1 person, 147.2ms\n",
      "Speed: 3.0ms preprocess, 147.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Prediction: Focused (Confidence: 0.02)\n",
      "\n",
      "0: 480x640 1 person, 146.4ms\n",
      "Speed: 3.0ms preprocess, 146.4ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 149.0ms\n",
      "Speed: 3.0ms preprocess, 149.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 146.6ms\n",
      "Speed: 3.5ms preprocess, 146.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 154.2ms\n",
      "Speed: 3.0ms preprocess, 154.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 1 tie, 147.4ms\n",
      "Speed: 2.5ms preprocess, 147.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 156.0ms\n",
      "Speed: 3.0ms preprocess, 156.0ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 148.5ms\n",
      "Speed: 3.0ms preprocess, 148.5ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 157.4ms\n",
      "Speed: 2.5ms preprocess, 157.4ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 155.9ms\n",
      "Speed: 3.0ms preprocess, 155.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Prediction: Focused (Confidence: 0.01)\n",
      "\n",
      "0: 480x640 1 person, 167.6ms\n",
      "Speed: 4.0ms preprocess, 167.6ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Focused (Confidence: 0.02)\n",
      "\n",
      "0: 480x640 1 person, 148.9ms\n",
      "Speed: 2.0ms preprocess, 148.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Prediction: Focused (Confidence: 0.02)\n",
      "\n",
      "0: 480x640 1 person, 164.1ms\n",
      "Speed: 4.0ms preprocess, 164.1ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Prediction: Focused (Confidence: 0.05)\n",
      "\n",
      "0: 480x640 1 person, 149.4ms\n",
      "Speed: 3.5ms preprocess, 149.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Prediction: Focused (Confidence: 0.02)\n",
      "\n",
      "0: 480x640 1 person, 170.0ms\n",
      "Speed: 3.0ms preprocess, 170.0ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Prediction: Focused (Confidence: 0.03)\n",
      "\n",
      "0: 480x640 1 person, 143.9ms\n",
      "Speed: 2.5ms preprocess, 143.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "Prediction: Focused (Confidence: 0.01)\n",
      "\n",
      "0: 480x640 1 person, 158.0ms\n",
      "Speed: 2.5ms preprocess, 158.0ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Focused (Confidence: 0.04)\n",
      "\n",
      "0: 480x640 1 person, 151.9ms\n",
      "Speed: 3.0ms preprocess, 151.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Prediction: Focused (Confidence: 0.01)\n",
      "\n",
      "0: 480x640 1 person, 165.1ms\n",
      "Speed: 3.0ms preprocess, 165.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 147.9ms\n",
      "Speed: 3.0ms preprocess, 147.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 150.6ms\n",
      "Speed: 4.0ms preprocess, 150.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 149.5ms\n",
      "Speed: 4.0ms preprocess, 149.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 145.4ms\n",
      "Speed: 3.0ms preprocess, 145.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Prediction: Focused (Confidence: 0.01)\n",
      "\n",
      "0: 480x640 1 person, 142.9ms\n",
      "Speed: 2.5ms preprocess, 142.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Prediction: Focused (Confidence: 0.01)\n",
      "\n",
      "0: 480x640 1 person, 148.7ms\n",
      "Speed: 3.0ms preprocess, 148.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Prediction: Focused (Confidence: 0.01)\n",
      "\n",
      "0: 480x640 1 person, 145.4ms\n",
      "Speed: 3.5ms preprocess, 145.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Focused (Confidence: 0.01)\n",
      "\n",
      "0: 480x640 1 person, 145.9ms\n",
      "Speed: 2.0ms preprocess, 145.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Prediction: Focused (Confidence: 0.01)\n",
      "\n",
      "0: 480x640 1 person, 152.9ms\n",
      "Speed: 3.0ms preprocess, 152.9ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Focused (Confidence: 0.01)\n",
      "\n",
      "0: 480x640 1 person, 150.6ms\n",
      "Speed: 3.0ms preprocess, 150.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Focused (Confidence: 0.01)\n",
      "\n",
      "0: 480x640 1 person, 145.4ms\n",
      "Speed: 3.0ms preprocess, 145.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Prediction: Focused (Confidence: 0.01)\n",
      "\n",
      "0: 480x640 1 person, 150.9ms\n",
      "Speed: 3.0ms preprocess, 150.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Prediction: Focused (Confidence: 0.01)\n",
      "\n",
      "0: 480x640 1 person, 1 tie, 160.4ms\n",
      "Speed: 3.6ms preprocess, 160.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Prediction: Focused (Confidence: 0.02)\n",
      "\n",
      "0: 480x640 1 person, 167.2ms\n",
      "Speed: 2.5ms preprocess, 167.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Focused (Confidence: 0.03)\n",
      "\n",
      "0: 480x640 1 person, 149.2ms\n",
      "Speed: 2.5ms preprocess, 149.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Prediction: Focused (Confidence: 0.02)\n",
      "\n",
      "0: 480x640 1 person, 150.1ms\n",
      "Speed: 2.5ms preprocess, 150.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Focused (Confidence: 0.01)\n",
      "\n",
      "0: 480x640 1 person, 162.5ms\n",
      "Speed: 2.0ms preprocess, 162.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Focused (Confidence: 0.02)\n",
      "\n",
      "0: 480x640 1 person, 141.9ms\n",
      "Speed: 4.0ms preprocess, 141.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Focused (Confidence: 0.06)\n",
      "\n",
      "0: 480x640 1 person, 142.9ms\n",
      "Speed: 2.5ms preprocess, 142.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Focused (Confidence: 0.05)\n",
      "\n",
      "0: 480x640 1 person, 144.9ms\n",
      "Speed: 3.5ms preprocess, 144.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Prediction: Focused (Confidence: 0.03)\n",
      "\n",
      "0: 480x640 1 person, 147.2ms\n",
      "Speed: 3.0ms preprocess, 147.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Prediction: Focused (Confidence: 0.02)\n",
      "\n",
      "0: 480x640 1 person, 168.9ms\n",
      "Speed: 2.5ms preprocess, 168.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Focused (Confidence: 0.02)\n",
      "\n",
      "0: 480x640 1 person, 143.9ms\n",
      "Speed: 3.0ms preprocess, 143.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Focused (Confidence: 0.02)\n",
      "\n",
      "0: 480x640 1 person, 149.9ms\n",
      "Speed: 3.0ms preprocess, 149.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Focused (Confidence: 0.01)\n",
      "\n",
      "0: 480x640 1 person, 151.5ms\n",
      "Speed: 3.5ms preprocess, 151.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 156.9ms\n",
      "Speed: 3.5ms preprocess, 156.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 1 tie, 142.2ms\n",
      "Speed: 4.0ms preprocess, 142.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No face detected - Student is distracted\n",
      "\n",
      "0: 480x640 1 person, 145.4ms\n",
      "Speed: 3.0ms preprocess, 145.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 149.9ms\n",
      "Speed: 3.0ms preprocess, 149.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 2 persons, 143.9ms\n",
      "Speed: 3.0ms preprocess, 143.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No face detected - Student is distracted\n",
      "\n",
      "0: 480x640 1 person, 143.4ms\n",
      "Speed: 3.0ms preprocess, 143.4ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No face detected - Student is distracted\n",
      "\n",
      "0: 480x640 2 persons, 142.9ms\n",
      "Speed: 4.0ms preprocess, 142.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No face detected - Student is distracted\n",
      "\n",
      "0: 480x640 1 person, 143.4ms\n",
      "Speed: 3.0ms preprocess, 143.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No face detected - Student is distracted\n",
      "\n",
      "0: 480x640 1 person, 147.8ms\n",
      "Speed: 3.0ms preprocess, 147.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No face detected - Student is distracted\n",
      "\n",
      "0: 480x640 2 persons, 144.3ms\n",
      "Speed: 3.0ms preprocess, 144.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No face detected - Student is distracted\n",
      "\n",
      "0: 480x640 2 persons, 146.2ms\n",
      "Speed: 3.0ms preprocess, 146.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No face detected - Student is distracted\n",
      "\n",
      "0: 480x640 1 person, 143.9ms\n",
      "Speed: 3.5ms preprocess, 143.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No face detected - Student is distracted\n",
      "\n",
      "0: 480x640 1 person, 146.9ms\n",
      "Speed: 2.6ms preprocess, 146.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No face detected - Student is distracted\n",
      "\n",
      "0: 480x640 1 person, 145.9ms\n",
      "Speed: 2.0ms preprocess, 145.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No face detected - Student is distracted\n",
      "\n",
      "0: 480x640 1 person, 148.3ms\n",
      "Speed: 3.0ms preprocess, 148.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 147.4ms\n",
      "Speed: 2.0ms preprocess, 147.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 140.9ms\n",
      "Speed: 2.5ms preprocess, 140.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 155.5ms\n",
      "Speed: 3.0ms preprocess, 155.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 148.4ms\n",
      "Speed: 3.0ms preprocess, 148.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 1 tie, 151.9ms\n",
      "Speed: 3.0ms preprocess, 151.9ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 150.3ms\n",
      "Speed: 3.0ms preprocess, 150.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 149.4ms\n",
      "Speed: 2.0ms preprocess, 149.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 153.2ms\n",
      "Speed: 3.0ms preprocess, 153.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 155.5ms\n",
      "Speed: 3.5ms preprocess, 155.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 152.6ms\n",
      "Speed: 3.0ms preprocess, 152.6ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 1 tv, 149.9ms\n",
      "Speed: 3.0ms preprocess, 149.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 151.9ms\n",
      "Speed: 2.5ms preprocess, 151.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 157.5ms\n",
      "Speed: 3.0ms preprocess, 157.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 152.6ms\n",
      "Speed: 3.0ms preprocess, 152.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 151.4ms\n",
      "Speed: 3.0ms preprocess, 151.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 152.6ms\n",
      "Speed: 2.5ms preprocess, 152.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 154.9ms\n",
      "Speed: 3.5ms preprocess, 154.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 150.9ms\n",
      "Speed: 4.0ms preprocess, 150.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 146.4ms\n",
      "Speed: 3.0ms preprocess, 146.4ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 150.3ms\n",
      "Speed: 3.0ms preprocess, 150.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 153.4ms\n",
      "Speed: 3.0ms preprocess, 153.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 150.9ms\n",
      "Speed: 3.0ms preprocess, 150.9ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 153.4ms\n",
      "Speed: 3.6ms preprocess, 153.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 152.4ms\n",
      "Speed: 2.6ms preprocess, 152.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 162.0ms\n",
      "Speed: 3.5ms preprocess, 162.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 153.6ms\n",
      "Speed: 4.0ms preprocess, 153.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 148.6ms\n",
      "Speed: 3.5ms preprocess, 148.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 156.2ms\n",
      "Speed: 3.0ms preprocess, 156.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 151.8ms\n",
      "Speed: 2.5ms preprocess, 151.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 148.0ms\n",
      "Speed: 3.0ms preprocess, 148.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No face detected - Student is distracted\n",
      "\n",
      "0: 480x640 1 person, 152.8ms\n",
      "Speed: 2.0ms preprocess, 152.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 150.9ms\n",
      "Speed: 3.0ms preprocess, 150.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 154.7ms\n",
      "Speed: 3.0ms preprocess, 154.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 150.9ms\n",
      "Speed: 3.0ms preprocess, 150.9ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 1 tie, 153.2ms\n",
      "Speed: 3.0ms preprocess, 153.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 149.6ms\n",
      "Speed: 3.0ms preprocess, 149.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 154.4ms\n",
      "Speed: 2.0ms preprocess, 154.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 140.8ms\n",
      "Speed: 3.0ms preprocess, 140.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 160.9ms\n",
      "Speed: 3.0ms preprocess, 160.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Focused (Confidence: 0.08)\n",
      "\n",
      "0: 480x640 1 person, 161.5ms\n",
      "Speed: 2.0ms preprocess, 161.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Focused (Confidence: 0.35)\n",
      "\n",
      "0: 480x640 1 person, 151.9ms\n",
      "Speed: 3.5ms preprocess, 151.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Focused (Confidence: 0.50)\n",
      "\n",
      "0: 480x640 1 person, 164.0ms\n",
      "Speed: 4.0ms preprocess, 164.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Focused (Confidence: 0.50)\n",
      "\n",
      "0: 480x640 1 person, 147.5ms\n",
      "Speed: 2.5ms preprocess, 147.5ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Focused (Confidence: 0.31)\n",
      "\n",
      "0: 480x640 1 person, 159.5ms\n",
      "Speed: 2.5ms preprocess, 159.5ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Focused (Confidence: 0.44)\n",
      "\n",
      "0: 480x640 1 person, 157.4ms\n",
      "Speed: 3.0ms preprocess, 157.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Focused (Confidence: 0.34)\n",
      "\n",
      "0: 480x640 1 person, 165.9ms\n",
      "Speed: 2.5ms preprocess, 165.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Focused (Confidence: 0.43)\n",
      "\n",
      "0: 480x640 1 person, 145.0ms\n",
      "Speed: 2.5ms preprocess, 145.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Focused (Confidence: 0.39)\n",
      "\n",
      "0: 480x640 1 person, 173.2ms\n",
      "Speed: 2.5ms preprocess, 173.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "Prediction: Focused (Confidence: 0.32)\n",
      "\n",
      "0: 480x640 1 person, 159.4ms\n",
      "Speed: 2.0ms preprocess, 159.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Prediction: Focused (Confidence: 0.27)\n",
      "\n",
      "0: 480x640 1 person, 152.5ms\n",
      "Speed: 1.5ms preprocess, 152.5ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Prediction: Focused (Confidence: 0.23)\n",
      "\n",
      "0: 480x640 1 person, 165.5ms\n",
      "Speed: 3.0ms preprocess, 165.5ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Focused (Confidence: 0.32)\n",
      "\n",
      "0: 480x640 1 person, 157.2ms\n",
      "Speed: 3.0ms preprocess, 157.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Prediction: Focused (Confidence: 0.33)\n",
      "\n",
      "0: 480x640 1 person, 165.8ms\n",
      "Speed: 3.5ms preprocess, 165.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Prediction: Focused (Confidence: 0.44)\n",
      "\n",
      "0: 480x640 1 person, 149.4ms\n",
      "Speed: 3.6ms preprocess, 149.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "Prediction: Focused (Confidence: 0.27)\n",
      "\n",
      "0: 480x640 1 person, 157.4ms\n",
      "Speed: 3.0ms preprocess, 157.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Prediction: Focused (Confidence: 0.13)\n",
      "\n",
      "0: 480x640 1 person, 163.5ms\n",
      "Speed: 4.5ms preprocess, 163.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Focused (Confidence: 0.13)\n",
      "\n",
      "0: 480x640 1 person, 167.5ms\n",
      "Speed: 3.0ms preprocess, 167.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Focused (Confidence: 0.16)\n",
      "\n",
      "0: 480x640 1 person, 160.0ms\n",
      "Speed: 3.0ms preprocess, 160.0ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "Prediction: Focused (Confidence: 0.23)\n",
      "\n",
      "0: 480x640 1 person, 150.4ms\n",
      "Speed: 4.4ms preprocess, 150.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Prediction: Focused (Confidence: 0.14)\n",
      "\n",
      "0: 480x640 1 person, 147.2ms\n",
      "Speed: 2.0ms preprocess, 147.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Focused (Confidence: 0.13)\n",
      "\n",
      "0: 480x640 1 person, 146.4ms\n",
      "Speed: 3.0ms preprocess, 146.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Focused (Confidence: 0.11)\n",
      "\n",
      "0: 480x640 1 person, 1 oven, 147.9ms\n",
      "Speed: 3.0ms preprocess, 147.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Focused (Confidence: 0.12)\n",
      "\n",
      "0: 480x640 1 person, 151.4ms\n",
      "Speed: 2.0ms preprocess, 151.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Focused (Confidence: 0.01)\n",
      "\n",
      "0: 480x640 1 person, 152.5ms\n",
      "Speed: 3.5ms preprocess, 152.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 1 tie, 150.1ms\n",
      "Speed: 2.5ms preprocess, 150.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 143.9ms\n",
      "Speed: 3.5ms preprocess, 143.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 149.1ms\n",
      "Speed: 3.0ms preprocess, 149.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 145.9ms\n",
      "Speed: 2.5ms preprocess, 145.9ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 142.4ms\n",
      "Speed: 3.0ms preprocess, 142.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 146.4ms\n",
      "Speed: 2.0ms preprocess, 146.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Prediction: Focused (Confidence: 0.01)\n",
      "\n",
      "0: 480x640 1 person, 1 tie, 146.7ms\n",
      "Speed: 3.5ms preprocess, 146.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 1 tie, 151.9ms\n",
      "Speed: 2.0ms preprocess, 151.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 150.4ms\n",
      "Speed: 2.0ms preprocess, 150.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 154.0ms\n",
      "Speed: 5.0ms preprocess, 154.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 154.4ms\n",
      "Speed: 3.0ms preprocess, 154.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 153.9ms\n",
      "Speed: 3.0ms preprocess, 153.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Prediction: Focused (Confidence: 0.04)\n",
      "\n",
      "0: 480x640 1 person, 153.4ms\n",
      "Speed: 3.5ms preprocess, 153.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Focused (Confidence: 0.04)\n",
      "\n",
      "0: 480x640 1 person, 150.4ms\n",
      "Speed: 3.5ms preprocess, 150.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Focused (Confidence: 0.01)\n",
      "\n",
      "0: 480x640 1 person, 151.1ms\n",
      "Speed: 3.0ms preprocess, 151.1ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 148.3ms\n",
      "Speed: 3.4ms preprocess, 148.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No face detected - Student is distracted\n",
      "\n",
      "0: 480x640 1 person, 153.1ms\n",
      "Speed: 3.5ms preprocess, 153.1ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No face detected - Student is distracted\n",
      "\n",
      "0: 480x640 1 person, 153.2ms\n",
      "Speed: 3.0ms preprocess, 153.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No face detected - Student is distracted\n",
      "\n",
      "0: 480x640 1 person, 153.9ms\n",
      "Speed: 3.0ms preprocess, 153.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 154.6ms\n",
      "Speed: 4.0ms preprocess, 154.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 (no detections), 154.9ms\n",
      "Speed: 3.0ms preprocess, 154.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 157.5ms\n",
      "Speed: 3.0ms preprocess, 157.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 161.0ms\n",
      "Speed: 3.5ms preprocess, 161.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 175.2ms\n",
      "Speed: 3.5ms preprocess, 175.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 161.5ms\n",
      "Speed: 3.5ms preprocess, 161.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 160.5ms\n",
      "Speed: 3.0ms preprocess, 160.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 158.5ms\n",
      "Speed: 2.0ms preprocess, 158.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 148.4ms\n",
      "Speed: 3.0ms preprocess, 148.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 153.5ms\n",
      "Speed: 3.0ms preprocess, 153.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 148.4ms\n",
      "Speed: 3.0ms preprocess, 148.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 151.9ms\n",
      "Speed: 3.0ms preprocess, 151.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 148.9ms\n",
      "Speed: 3.0ms preprocess, 148.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 149.2ms\n",
      "Speed: 3.0ms preprocess, 149.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 155.3ms\n",
      "Speed: 2.0ms preprocess, 155.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 169.0ms\n",
      "Speed: 2.0ms preprocess, 169.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 151.9ms\n",
      "Speed: 3.0ms preprocess, 151.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 151.1ms\n",
      "Speed: 2.5ms preprocess, 151.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 153.0ms\n",
      "Speed: 2.5ms preprocess, 153.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 150.9ms\n",
      "Speed: 2.0ms preprocess, 150.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 152.9ms\n",
      "Speed: 2.0ms preprocess, 152.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 148.4ms\n",
      "Speed: 3.5ms preprocess, 148.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 155.5ms\n",
      "Speed: 2.5ms preprocess, 155.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 148.9ms\n",
      "Speed: 2.0ms preprocess, 148.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 153.1ms\n",
      "Speed: 2.0ms preprocess, 153.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 150.4ms\n",
      "Speed: 3.0ms preprocess, 150.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 150.9ms\n",
      "Speed: 2.0ms preprocess, 150.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 150.0ms\n",
      "Speed: 3.5ms preprocess, 150.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 (no detections), 150.0ms\n",
      "Speed: 1.5ms preprocess, 150.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 153.4ms\n",
      "Speed: 2.0ms preprocess, 153.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 147.4ms\n",
      "Speed: 2.5ms preprocess, 147.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 154.5ms\n",
      "Speed: 3.0ms preprocess, 154.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 149.4ms\n",
      "Speed: 3.0ms preprocess, 149.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 152.5ms\n",
      "Speed: 3.0ms preprocess, 152.5ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 146.4ms\n",
      "Speed: 3.5ms preprocess, 146.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 148.4ms\n",
      "Speed: 3.5ms preprocess, 148.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 150.8ms\n",
      "Speed: 3.0ms preprocess, 150.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 157.4ms\n",
      "Speed: 3.0ms preprocess, 157.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 158.0ms\n",
      "Speed: 5.0ms preprocess, 158.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 156.3ms\n",
      "Speed: 2.0ms preprocess, 156.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 151.1ms\n",
      "Speed: 3.0ms preprocess, 151.1ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 152.4ms\n",
      "Speed: 2.5ms preprocess, 152.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 146.9ms\n",
      "Speed: 3.0ms preprocess, 146.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 154.0ms\n",
      "Speed: 3.0ms preprocess, 154.0ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 155.6ms\n",
      "Speed: 2.5ms preprocess, 155.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 157.9ms\n",
      "Speed: 2.0ms preprocess, 157.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 150.0ms\n",
      "Speed: 3.0ms preprocess, 150.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 161.0ms\n",
      "Speed: 2.5ms preprocess, 161.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 144.4ms\n",
      "Speed: 3.5ms preprocess, 144.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 151.9ms\n",
      "Speed: 3.0ms preprocess, 151.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 143.4ms\n",
      "Speed: 2.5ms preprocess, 143.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 145.4ms\n",
      "Speed: 4.0ms preprocess, 145.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 146.3ms\n",
      "Speed: 2.0ms preprocess, 146.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 143.4ms\n",
      "Speed: 3.5ms preprocess, 143.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 147.3ms\n",
      "Speed: 2.0ms preprocess, 147.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 148.9ms\n",
      "Speed: 4.0ms preprocess, 148.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 146.3ms\n",
      "Speed: 3.0ms preprocess, 146.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 144.4ms\n",
      "Speed: 3.0ms preprocess, 144.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 143.4ms\n",
      "Speed: 2.0ms preprocess, 143.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 157.4ms\n",
      "Speed: 3.0ms preprocess, 157.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 158.4ms\n",
      "Speed: 2.0ms preprocess, 158.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 154.9ms\n",
      "Speed: 5.0ms preprocess, 154.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 148.8ms\n",
      "Speed: 2.6ms preprocess, 148.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 147.8ms\n",
      "Speed: 2.6ms preprocess, 147.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 169.9ms\n",
      "Speed: 2.6ms preprocess, 169.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 166.0ms\n",
      "Speed: 2.5ms preprocess, 166.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 157.8ms\n",
      "Speed: 2.0ms preprocess, 157.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 149.7ms\n",
      "Speed: 2.0ms preprocess, 149.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 139.9ms\n",
      "Speed: 2.0ms preprocess, 139.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 145.4ms\n",
      "Speed: 2.5ms preprocess, 145.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 147.4ms\n",
      "Speed: 3.0ms preprocess, 147.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 140.4ms\n",
      "Speed: 2.0ms preprocess, 140.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 153.5ms\n",
      "Speed: 3.0ms preprocess, 153.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 (no detections), 162.7ms\n",
      "Speed: 3.5ms preprocess, 162.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 169.0ms\n",
      "Speed: 3.0ms preprocess, 169.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 143.4ms\n",
      "Speed: 2.0ms preprocess, 143.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 147.2ms\n",
      "Speed: 2.0ms preprocess, 147.2ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 147.4ms\n",
      "Speed: 2.5ms preprocess, 147.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 161.0ms\n",
      "Speed: 3.0ms preprocess, 161.0ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 144.4ms\n",
      "Speed: 3.0ms preprocess, 144.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 149.9ms\n",
      "Speed: 2.5ms preprocess, 149.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 293.0ms\n",
      "Speed: 2.5ms preprocess, 293.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 157.5ms\n",
      "Speed: 4.5ms preprocess, 157.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 167.5ms\n",
      "Speed: 4.5ms preprocess, 167.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 163.8ms\n",
      "Speed: 3.0ms preprocess, 163.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 164.1ms\n",
      "Speed: 2.5ms preprocess, 164.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 153.2ms\n",
      "Speed: 2.0ms preprocess, 153.2ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 169.1ms\n",
      "Speed: 2.5ms preprocess, 169.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 153.6ms\n",
      "Speed: 2.5ms preprocess, 153.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 160.5ms\n",
      "Speed: 4.0ms preprocess, 160.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 163.6ms\n",
      "Speed: 2.0ms preprocess, 163.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 159.1ms\n",
      "Speed: 3.0ms preprocess, 159.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 159.3ms\n",
      "Speed: 3.0ms preprocess, 159.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 150.3ms\n",
      "Speed: 2.0ms preprocess, 150.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 153.8ms\n",
      "Speed: 2.0ms preprocess, 153.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 159.5ms\n",
      "Speed: 2.0ms preprocess, 159.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 154.4ms\n",
      "Speed: 3.0ms preprocess, 154.4ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 157.5ms\n",
      "Speed: 2.5ms preprocess, 157.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 161.8ms\n",
      "Speed: 3.0ms preprocess, 161.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 155.1ms\n",
      "Speed: 3.6ms preprocess, 155.1ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 160.0ms\n",
      "Speed: 3.0ms preprocess, 160.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 154.0ms\n",
      "Speed: 3.5ms preprocess, 154.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 152.4ms\n",
      "Speed: 3.0ms preprocess, 152.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 159.0ms\n",
      "Speed: 3.0ms preprocess, 159.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 157.4ms\n",
      "Speed: 2.5ms preprocess, 157.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 154.2ms\n",
      "Speed: 3.2ms preprocess, 154.2ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 149.9ms\n",
      "Speed: 2.6ms preprocess, 149.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 153.9ms\n",
      "Speed: 2.5ms preprocess, 153.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 154.5ms\n",
      "Speed: 4.5ms preprocess, 154.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 155.4ms\n",
      "Speed: 3.0ms preprocess, 155.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 148.4ms\n",
      "Speed: 3.0ms preprocess, 148.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 159.4ms\n",
      "Speed: 3.0ms preprocess, 159.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 151.9ms\n",
      "Speed: 2.0ms preprocess, 151.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 153.6ms\n",
      "Speed: 4.0ms preprocess, 153.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 162.7ms\n",
      "Speed: 2.0ms preprocess, 162.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 151.4ms\n",
      "Speed: 3.0ms preprocess, 151.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 149.9ms\n",
      "Speed: 3.5ms preprocess, 149.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 150.0ms\n",
      "Speed: 2.5ms preprocess, 150.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 (no detections), 141.8ms\n",
      "Speed: 3.1ms preprocess, 141.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 154.8ms\n",
      "Speed: 2.0ms preprocess, 154.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 141.9ms\n",
      "Speed: 2.5ms preprocess, 141.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 145.4ms\n",
      "Speed: 2.0ms preprocess, 145.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 146.7ms\n",
      "Speed: 3.5ms preprocess, 146.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 144.2ms\n",
      "Speed: 1.0ms preprocess, 144.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 150.5ms\n",
      "Speed: 3.0ms preprocess, 150.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 141.9ms\n",
      "Speed: 2.5ms preprocess, 141.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 148.4ms\n",
      "Speed: 2.5ms preprocess, 148.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 144.0ms\n",
      "Speed: 2.5ms preprocess, 144.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 144.3ms\n",
      "Speed: 2.0ms preprocess, 144.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 152.0ms\n",
      "Speed: 2.0ms preprocess, 152.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 143.4ms\n",
      "Speed: 2.5ms preprocess, 143.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 156.9ms\n",
      "Speed: 2.0ms preprocess, 156.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 154.4ms\n",
      "Speed: 4.0ms preprocess, 154.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 145.4ms\n",
      "Speed: 2.5ms preprocess, 145.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 147.4ms\n",
      "Speed: 3.0ms preprocess, 147.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 141.2ms\n",
      "Speed: 3.0ms preprocess, 141.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 148.3ms\n",
      "Speed: 2.0ms preprocess, 148.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 150.4ms\n",
      "Speed: 2.0ms preprocess, 150.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 146.7ms\n",
      "Speed: 2.0ms preprocess, 146.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 153.5ms\n",
      "Speed: 2.0ms preprocess, 153.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 159.7ms\n",
      "Speed: 3.5ms preprocess, 159.7ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 147.4ms\n",
      "Speed: 2.0ms preprocess, 147.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 144.9ms\n",
      "Speed: 3.5ms preprocess, 144.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 142.8ms\n",
      "Speed: 2.0ms preprocess, 142.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 143.9ms\n",
      "Speed: 2.5ms preprocess, 143.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 145.9ms\n",
      "Speed: 3.0ms preprocess, 145.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 151.4ms\n",
      "Speed: 2.0ms preprocess, 151.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 144.9ms\n",
      "Speed: 2.0ms preprocess, 144.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 141.9ms\n",
      "Speed: 3.5ms preprocess, 141.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 145.4ms\n",
      "Speed: 2.5ms preprocess, 145.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 143.3ms\n",
      "Speed: 2.5ms preprocess, 143.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 146.4ms\n",
      "Speed: 2.0ms preprocess, 146.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 142.3ms\n",
      "Speed: 3.0ms preprocess, 142.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 146.6ms\n",
      "Speed: 3.6ms preprocess, 146.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 148.5ms\n",
      "Speed: 2.0ms preprocess, 148.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 (no detections), 144.5ms\n",
      "Speed: 2.0ms preprocess, 144.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 1 person, 147.9ms\n",
      "Speed: 3.5ms preprocess, 147.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No face detected - Student is distracted\n",
      "\n",
      "0: 480x640 (no detections), 147.1ms\n",
      "Speed: 3.0ms preprocess, 147.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No person detected - Adding to distraction time\n",
      "\n",
      "0: 480x640 1 person, 143.5ms\n",
      "Speed: 3.0ms preprocess, 143.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No face detected - Student is distracted\n",
      "\n",
      "0: 480x640 2 persons, 147.9ms\n",
      "Speed: 3.5ms preprocess, 147.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No face detected - Student is distracted\n",
      "\n",
      "0: 480x640 1 person, 143.4ms\n",
      "Speed: 4.0ms preprocess, 143.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No face detected - Student is distracted\n",
      "\n",
      "0: 480x640 1 person, 152.4ms\n",
      "Speed: 2.0ms preprocess, 152.4ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No face detected - Student is distracted\n",
      "\n",
      "0: 480x640 1 person, 146.4ms\n",
      "Speed: 3.0ms preprocess, 146.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 150.4ms\n",
      "Speed: 3.0ms preprocess, 150.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 147.4ms\n",
      "Speed: 3.0ms preprocess, 147.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No face detected - Student is distracted\n",
      "\n",
      "0: 480x640 1 person, 146.8ms\n",
      "Speed: 2.0ms preprocess, 146.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Prediction: Focused (Confidence: 0.01)\n",
      "\n",
      "0: 480x640 1 person, 145.5ms\n",
      "Speed: 2.5ms preprocess, 145.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 144.1ms\n",
      "Speed: 3.0ms preprocess, 144.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 144.4ms\n",
      "Speed: 3.0ms preprocess, 144.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Prediction: Focused (Confidence: 0.12)\n",
      "\n",
      "0: 480x640 1 person, 142.3ms\n",
      "Speed: 2.0ms preprocess, 142.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Focused (Confidence: 0.13)\n",
      "\n",
      "0: 480x640 1 person, 149.0ms\n",
      "Speed: 4.5ms preprocess, 149.0ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Focused (Confidence: 0.24)\n",
      "\n",
      "0: 480x640 1 person, 141.4ms\n",
      "Speed: 4.0ms preprocess, 141.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Focused (Confidence: 0.38)\n",
      "\n",
      "0: 480x640 1 person, 144.9ms\n",
      "Speed: 2.5ms preprocess, 144.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Focused (Confidence: 0.32)\n",
      "\n",
      "0: 480x640 1 person, 145.4ms\n",
      "Speed: 2.0ms preprocess, 145.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Focused (Confidence: 0.15)\n",
      "\n",
      "0: 480x640 1 person, 155.4ms\n",
      "Speed: 2.6ms preprocess, 155.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Focused (Confidence: 0.49)\n",
      "\n",
      "0: 480x640 1 person, 154.2ms\n",
      "Speed: 3.0ms preprocess, 154.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Focused (Confidence: 0.41)\n",
      "\n",
      "0: 480x640 1 person, 155.0ms\n",
      "Speed: 2.5ms preprocess, 155.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Distracted (Confidence: 0.60)\n",
      "\n",
      "0: 480x640 1 person, 150.9ms\n",
      "Speed: 3.0ms preprocess, 150.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 150.0ms\n",
      "Speed: 3.5ms preprocess, 150.0ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Distracted (Confidence: 0.63)\n",
      "\n",
      "0: 480x640 1 person, 151.9ms\n",
      "Speed: 4.0ms preprocess, 151.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Prediction: Focused (Confidence: 0.33)\n",
      "\n",
      "0: 480x640 1 person, 156.9ms\n",
      "Speed: 3.5ms preprocess, 156.9ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Prediction: Focused (Confidence: 0.46)\n",
      "\n",
      "0: 480x640 1 person, 1 keyboard, 155.5ms\n",
      "Speed: 3.0ms preprocess, 155.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Distracted (Confidence: 0.98)\n",
      "\n",
      "0: 480x640 1 person, 161.0ms\n",
      "Speed: 3.0ms preprocess, 161.0ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 1 keyboard, 156.4ms\n",
      "Speed: 2.5ms preprocess, 156.4ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 174.8ms\n",
      "Speed: 3.5ms preprocess, 174.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 154.5ms\n",
      "Speed: 3.5ms preprocess, 154.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 150.4ms\n",
      "Speed: 3.0ms preprocess, 150.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 153.9ms\n",
      "Speed: 3.5ms preprocess, 153.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 156.5ms\n",
      "Speed: 2.0ms preprocess, 156.5ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 1 tie, 154.5ms\n",
      "Speed: 3.0ms preprocess, 154.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Focused (Confidence: 0.40)\n",
      "\n",
      "0: 480x640 1 person, 1 tie, 159.4ms\n",
      "Speed: 4.0ms preprocess, 159.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Focused (Confidence: 0.03)\n",
      "\n",
      "0: 480x640 1 person, 153.4ms\n",
      "Speed: 3.0ms preprocess, 153.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 165.0ms\n",
      "Speed: 3.0ms preprocess, 165.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Focused (Confidence: 0.09)\n",
      "\n",
      "0: 480x640 1 person, 162.5ms\n",
      "Speed: 3.0ms preprocess, 162.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Distracted (Confidence: 0.82)\n",
      "\n",
      "0: 480x640 1 person, 164.5ms\n",
      "Speed: 4.0ms preprocess, 164.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Prediction: Distracted (Confidence: 0.56)\n",
      "\n",
      "0: 480x640 1 person, 157.3ms\n",
      "Speed: 3.7ms preprocess, 157.3ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 159.3ms\n",
      "Speed: 3.0ms preprocess, 159.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 157.8ms\n",
      "Speed: 3.0ms preprocess, 157.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 155.0ms\n",
      "Speed: 3.0ms preprocess, 155.0ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 152.9ms\n",
      "Speed: 3.0ms preprocess, 152.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 157.3ms\n",
      "Speed: 4.0ms preprocess, 157.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 154.8ms\n",
      "Speed: 3.0ms preprocess, 154.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 153.0ms\n",
      "Speed: 4.5ms preprocess, 153.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 161.5ms\n",
      "Speed: 3.5ms preprocess, 161.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 154.5ms\n",
      "Speed: 4.5ms preprocess, 154.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 155.4ms\n",
      "Speed: 3.0ms preprocess, 155.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 153.8ms\n",
      "Speed: 5.0ms preprocess, 153.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 154.0ms\n",
      "Speed: 3.0ms preprocess, 154.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Focused (Confidence: 0.02)\n",
      "\n",
      "0: 480x640 1 person, 1 keyboard, 154.9ms\n",
      "Speed: 3.0ms preprocess, 154.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Focused (Confidence: 0.07)\n",
      "\n",
      "0: 480x640 1 person, 161.5ms\n",
      "Speed: 3.0ms preprocess, 161.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Focused (Confidence: 0.28)\n",
      "\n",
      "0: 480x640 1 person, 158.4ms\n",
      "Speed: 3.0ms preprocess, 158.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Distracted (Confidence: 0.73)\n",
      "\n",
      "0: 480x640 1 person, 1 laptop, 152.5ms\n",
      "Speed: 3.5ms preprocess, 152.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Distracted (Confidence: 0.61)\n",
      "\n",
      "0: 480x640 1 person, 1 laptop, 155.5ms\n",
      "Speed: 2.5ms preprocess, 155.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Distracted (Confidence: 0.67)\n",
      "\n",
      "0: 480x640 1 person, 151.4ms\n",
      "Speed: 4.0ms preprocess, 151.4ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Prediction: Focused (Confidence: 0.42)\n",
      "\n",
      "0: 480x640 1 person, 149.9ms\n",
      "Speed: 3.0ms preprocess, 149.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Focused (Confidence: 0.26)\n",
      "\n",
      "0: 480x640 1 person, 158.5ms\n",
      "Speed: 2.5ms preprocess, 158.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Distracted (Confidence: 0.59)\n",
      "\n",
      "0: 480x640 1 person, 152.4ms\n",
      "Speed: 2.5ms preprocess, 152.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Focused (Confidence: 0.44)\n",
      "\n",
      "0: 480x640 1 person, 166.5ms\n",
      "Speed: 4.5ms preprocess, 166.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Prediction: Distracted (Confidence: 0.57)\n",
      "\n",
      "0: 480x640 1 person, 145.6ms\n",
      "Speed: 2.0ms preprocess, 145.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Prediction: Distracted (Confidence: 0.53)\n",
      "\n",
      "0: 480x640 1 person, 148.4ms\n",
      "Speed: 3.0ms preprocess, 148.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Prediction: Distracted (Confidence: 0.64)\n",
      "\n",
      "0: 480x640 1 person, 153.8ms\n",
      "Speed: 2.5ms preprocess, 153.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Distracted (Confidence: 0.67)\n",
      "\n",
      "0: 480x640 1 person, 1 keyboard, 151.4ms\n",
      "Speed: 3.0ms preprocess, 151.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Prediction: Focused (Confidence: 0.47)\n",
      "\n",
      "0: 480x640 1 person, 1 keyboard, 149.4ms\n",
      "Speed: 3.0ms preprocess, 149.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Prediction: Focused (Confidence: 0.49)\n",
      "\n",
      "0: 480x640 1 person, 1 keyboard, 148.4ms\n",
      "Speed: 3.0ms preprocess, 148.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Focused (Confidence: 0.37)\n",
      "\n",
      "0: 480x640 1 person, 141.9ms\n",
      "Speed: 2.0ms preprocess, 141.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Prediction: Focused (Confidence: 0.44)\n",
      "\n",
      "0: 480x640 1 person, 151.4ms\n",
      "Speed: 2.5ms preprocess, 151.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Focused (Confidence: 0.27)\n",
      "\n",
      "0: 480x640 1 person, 149.4ms\n",
      "Speed: 3.0ms preprocess, 149.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Prediction: Focused (Confidence: 0.06)\n",
      "\n",
      "0: 480x640 1 person, 148.4ms\n",
      "Speed: 3.0ms preprocess, 148.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Prediction: Focused (Confidence: 0.19)\n",
      "\n",
      "0: 480x640 1 person, 151.4ms\n",
      "Speed: 2.0ms preprocess, 151.4ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Focused (Confidence: 0.07)\n",
      "\n",
      "0: 480x640 1 person, 149.8ms\n",
      "Speed: 3.0ms preprocess, 149.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Prediction: Focused (Confidence: 0.28)\n",
      "\n",
      "0: 480x640 1 person, 152.6ms\n",
      "Speed: 2.5ms preprocess, 152.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Focused (Confidence: 0.22)\n",
      "\n",
      "0: 480x640 1 person, 149.4ms\n",
      "Speed: 2.2ms preprocess, 149.4ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Focused (Confidence: 0.09)\n",
      "\n",
      "0: 480x640 1 person, 1 tie, 149.4ms\n",
      "Speed: 3.5ms preprocess, 149.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 143.4ms\n",
      "Speed: 3.5ms preprocess, 143.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Distracted (Confidence: 0.69)\n",
      "\n",
      "0: 480x640 1 person, 144.9ms\n",
      "Speed: 4.0ms preprocess, 144.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Distracted (Confidence: 0.75)\n",
      "\n",
      "0: 480x640 1 person, 144.4ms\n",
      "Speed: 3.0ms preprocess, 144.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 146.4ms\n",
      "Speed: 2.0ms preprocess, 146.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 151.9ms\n",
      "Speed: 3.5ms preprocess, 151.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 143.9ms\n",
      "Speed: 3.0ms preprocess, 143.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Prediction: Focused (Confidence: 0.02)\n",
      "\n",
      "0: 480x640 1 person, 149.9ms\n",
      "Speed: 3.1ms preprocess, 149.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Prediction: Distracted (Confidence: 0.61)\n",
      "\n",
      "0: 480x640 1 person, 152.4ms\n",
      "Speed: 3.6ms preprocess, 152.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Distracted (Confidence: 0.59)\n",
      "\n",
      "0: 480x640 1 person, 1 laptop, 146.4ms\n",
      "Speed: 2.5ms preprocess, 146.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Prediction: Distracted (Confidence: 0.80)\n",
      "\n",
      "0: 480x640 1 person, 146.9ms\n",
      "Speed: 3.0ms preprocess, 146.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Distracted (Confidence: 0.76)\n",
      "\n",
      "0: 480x640 1 person, 145.3ms\n",
      "Speed: 2.0ms preprocess, 145.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Prediction: Distracted (Confidence: 0.96)\n",
      "\n",
      "0: 480x640 1 person, 146.9ms\n",
      "Speed: 3.0ms preprocess, 146.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Prediction: Distracted (Confidence: 0.97)\n",
      "\n",
      "0: 480x640 1 person, 143.9ms\n",
      "Speed: 2.0ms preprocess, 143.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Prediction: Distracted (Confidence: 0.99)\n",
      "\n",
      "0: 480x640 1 person, 151.7ms\n",
      "Speed: 2.0ms preprocess, 151.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Distracted (Confidence: 0.99)\n",
      "\n",
      "0: 480x640 1 person, 150.0ms\n",
      "Speed: 3.0ms preprocess, 150.0ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Distracted (Confidence: 0.99)\n",
      "\n",
      "0: 480x640 1 person, 144.4ms\n",
      "Speed: 2.0ms preprocess, 144.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 148.2ms\n",
      "Speed: 3.0ms preprocess, 148.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 149.3ms\n",
      "Speed: 4.5ms preprocess, 149.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 148.4ms\n",
      "Speed: 3.0ms preprocess, 148.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "Prediction: Distracted (Confidence: 0.99)\n",
      "\n",
      "0: 480x640 1 person, 152.9ms\n",
      "Speed: 3.5ms preprocess, 152.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 149.9ms\n",
      "Speed: 2.0ms preprocess, 149.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Prediction: Distracted (Confidence: 0.99)\n",
      "\n",
      "0: 480x640 1 person, 148.4ms\n",
      "Speed: 2.0ms preprocess, 148.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Prediction: Distracted (Confidence: 0.99)\n",
      "\n",
      "0: 480x640 1 person, 147.4ms\n",
      "Speed: 3.0ms preprocess, 147.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Prediction: Distracted (Confidence: 0.99)\n",
      "\n",
      "0: 480x640 1 person, 150.9ms\n",
      "Speed: 2.0ms preprocess, 150.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Prediction: Distracted (Confidence: 0.99)\n",
      "\n",
      "0: 480x640 1 person, 150.0ms\n",
      "Speed: 3.5ms preprocess, 150.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Distracted (Confidence: 0.99)\n",
      "\n",
      "0: 480x640 1 person, 140.4ms\n",
      "Speed: 3.0ms preprocess, 140.4ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Prediction: Distracted (Confidence: 0.99)\n",
      "\n",
      "0: 480x640 1 person, 147.3ms\n",
      "Speed: 3.6ms preprocess, 147.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Distracted (Confidence: 0.99)\n",
      "\n",
      "0: 480x640 1 person, 149.4ms\n",
      "Speed: 2.0ms preprocess, 149.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Distracted (Confidence: 0.99)\n",
      "\n",
      "0: 480x640 1 person, 145.1ms\n",
      "Speed: 2.0ms preprocess, 145.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Prediction: Distracted (Confidence: 0.99)\n",
      "\n",
      "0: 480x640 1 person, 149.4ms\n",
      "Speed: 3.5ms preprocess, 149.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Distracted (Confidence: 0.99)\n",
      "\n",
      "0: 480x640 1 person, 147.5ms\n",
      "Speed: 3.0ms preprocess, 147.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Distracted (Confidence: 0.99)\n",
      "\n",
      "0: 480x640 1 person, 148.8ms\n",
      "Speed: 3.6ms preprocess, 148.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Prediction: Distracted (Confidence: 0.99)\n",
      "\n",
      "0: 480x640 1 person, 147.1ms\n",
      "Speed: 3.0ms preprocess, 147.1ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Distracted (Confidence: 0.99)\n",
      "\n",
      "0: 480x640 1 person, 151.5ms\n",
      "Speed: 4.0ms preprocess, 151.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 147.8ms\n",
      "Speed: 3.0ms preprocess, 147.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Distracted (Confidence: 0.99)\n",
      "\n",
      "0: 480x640 1 person, 144.5ms\n",
      "Speed: 3.0ms preprocess, 144.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 146.5ms\n",
      "Speed: 3.5ms preprocess, 146.5ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Prediction: Distracted (Confidence: 0.99)\n",
      "\n",
      "0: 480x640 1 person, 148.4ms\n",
      "Speed: 3.0ms preprocess, 148.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 148.4ms\n",
      "Speed: 3.0ms preprocess, 148.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 148.4ms\n",
      "Speed: 3.5ms preprocess, 148.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 146.9ms\n",
      "Speed: 2.5ms preprocess, 146.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Distracted (Confidence: 0.99)\n",
      "\n",
      "0: 480x640 1 person, 146.0ms\n",
      "Speed: 2.0ms preprocess, 146.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 149.4ms\n",
      "Speed: 3.0ms preprocess, 149.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 143.8ms\n",
      "Speed: 3.5ms preprocess, 143.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 146.8ms\n",
      "Speed: 3.5ms preprocess, 146.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 145.4ms\n",
      "Speed: 2.0ms preprocess, 145.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 149.1ms\n",
      "Speed: 3.0ms preprocess, 149.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 150.6ms\n",
      "Speed: 3.0ms preprocess, 150.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 150.4ms\n",
      "Speed: 3.5ms preprocess, 150.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 148.0ms\n",
      "Speed: 3.0ms preprocess, 148.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 1 keyboard, 146.6ms\n",
      "Speed: 3.0ms preprocess, 146.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 148.0ms\n",
      "Speed: 3.0ms preprocess, 148.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 147.5ms\n",
      "Speed: 4.0ms preprocess, 147.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 153.4ms\n",
      "Speed: 3.0ms preprocess, 153.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Focused (Confidence: 0.01)\n",
      "\n",
      "0: 480x640 1 person, 156.9ms\n",
      "Speed: 2.6ms preprocess, 156.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Distracted (Confidence: 0.96)\n",
      "\n",
      "0: 480x640 1 person, 145.3ms\n",
      "Speed: 3.0ms preprocess, 145.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Distracted (Confidence: 0.99)\n",
      "\n",
      "0: 480x640 1 person, 149.0ms\n",
      "Speed: 2.0ms preprocess, 149.0ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 146.5ms\n",
      "Speed: 3.0ms preprocess, 146.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 144.4ms\n",
      "Speed: 3.0ms preprocess, 144.4ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 147.8ms\n",
      "Speed: 2.0ms preprocess, 147.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Distracted (Confidence: 0.96)\n",
      "\n",
      "0: 480x640 1 person, 144.9ms\n",
      "Speed: 3.0ms preprocess, 144.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Distracted (Confidence: 0.99)\n",
      "\n",
      "0: 480x640 1 person, 144.4ms\n",
      "Speed: 3.0ms preprocess, 144.4ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Distracted (Confidence: 0.99)\n",
      "\n",
      "0: 480x640 1 person, 145.9ms\n",
      "Speed: 3.0ms preprocess, 145.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Distracted (Confidence: 0.99)\n",
      "\n",
      "0: 480x640 1 person, 151.4ms\n",
      "Speed: 3.0ms preprocess, 151.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 149.9ms\n",
      "Speed: 2.5ms preprocess, 149.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Prediction: Distracted (Confidence: 0.99)\n",
      "\n",
      "0: 480x640 1 person, 147.4ms\n",
      "Speed: 2.6ms preprocess, 147.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 149.9ms\n",
      "Speed: 3.0ms preprocess, 149.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Prediction: Distracted (Confidence: 0.97)\n",
      "\n",
      "0: 480x640 1 person, 147.1ms\n",
      "Speed: 3.0ms preprocess, 147.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Focused (Confidence: 0.44)\n",
      "\n",
      "0: 480x640 1 person, 149.4ms\n",
      "Speed: 2.5ms preprocess, 149.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Prediction: Focused (Confidence: 0.49)\n",
      "\n",
      "0: 480x640 1 person, 145.9ms\n",
      "Speed: 3.0ms preprocess, 145.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Distracted (Confidence: 0.69)\n",
      "\n",
      "0: 480x640 1 person, 146.0ms\n",
      "Speed: 3.5ms preprocess, 146.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Focused (Confidence: 0.11)\n",
      "\n",
      "0: 480x640 1 person, 147.6ms\n",
      "Speed: 3.0ms preprocess, 147.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Prediction: Focused (Confidence: 0.15)\n",
      "\n",
      "0: 480x640 1 person, 143.4ms\n",
      "Speed: 2.0ms preprocess, 143.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Focused (Confidence: 0.49)\n",
      "\n",
      "0: 480x640 1 person, 147.5ms\n",
      "Speed: 3.5ms preprocess, 147.5ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Focused (Confidence: 0.34)\n",
      "\n",
      "0: 480x640 1 person, 1 keyboard, 150.6ms\n",
      "Speed: 3.0ms preprocess, 150.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Focused (Confidence: 0.39)\n",
      "\n",
      "0: 480x640 1 person, 148.8ms\n",
      "Speed: 3.0ms preprocess, 148.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Distracted (Confidence: 0.85)\n",
      "\n",
      "0: 480x640 1 person, 149.3ms\n",
      "Speed: 3.0ms preprocess, 149.3ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Prediction: Distracted (Confidence: 0.92)\n",
      "\n",
      "0: 480x640 1 person, 149.9ms\n",
      "Speed: 3.0ms preprocess, 149.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Distracted (Confidence: 0.93)\n",
      "\n",
      "0: 480x640 1 person, 139.8ms\n",
      "Speed: 4.5ms preprocess, 139.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Prediction: Distracted (Confidence: 0.80)\n",
      "\n",
      "0: 480x640 1 person, 144.9ms\n",
      "Speed: 3.0ms preprocess, 144.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Prediction: Distracted (Confidence: 0.70)\n",
      "\n",
      "0: 480x640 1 person, 147.0ms\n",
      "Speed: 3.0ms preprocess, 147.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Distracted (Confidence: 0.83)\n",
      "\n",
      "0: 480x640 1 person, 152.9ms\n",
      "Speed: 3.4ms preprocess, 152.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Prediction: Distracted (Confidence: 0.88)\n",
      "\n",
      "0: 480x640 1 person, 150.6ms\n",
      "Speed: 3.0ms preprocess, 150.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Prediction: Distracted (Confidence: 0.86)\n",
      "\n",
      "0: 480x640 1 person, 151.0ms\n",
      "Speed: 3.0ms preprocess, 151.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Distracted (Confidence: 0.82)\n",
      "\n",
      "0: 480x640 1 person, 149.5ms\n",
      "Speed: 4.5ms preprocess, 149.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Distracted (Confidence: 0.79)\n",
      "\n",
      "0: 480x640 1 person, 150.0ms\n",
      "Speed: 3.0ms preprocess, 150.0ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Prediction: Focused (Confidence: 0.49)\n",
      "\n",
      "0: 480x640 1 person, 142.9ms\n",
      "Speed: 3.0ms preprocess, 142.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Prediction: Focused (Confidence: 0.01)\n",
      "\n",
      "0: 480x640 1 person, 144.4ms\n",
      "Speed: 3.0ms preprocess, 144.4ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 146.9ms\n",
      "Speed: 3.0ms preprocess, 146.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 144.9ms\n",
      "Speed: 2.0ms preprocess, 144.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 153.0ms\n",
      "Speed: 3.0ms preprocess, 153.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 153.4ms\n",
      "Speed: 2.0ms preprocess, 153.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 149.4ms\n",
      "Speed: 2.0ms preprocess, 149.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 152.0ms\n",
      "Speed: 2.0ms preprocess, 152.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Distracted (Confidence: 0.89)\n",
      "\n",
      "0: 480x640 1 person, 146.4ms\n",
      "Speed: 3.5ms preprocess, 146.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 141.8ms\n",
      "Speed: 3.0ms preprocess, 141.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Distracted (Confidence: 0.99)\n",
      "\n",
      "0: 480x640 1 person, 147.4ms\n",
      "Speed: 3.0ms preprocess, 147.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Distracted (Confidence: 0.98)\n",
      "\n",
      "0: 480x640 1 person, 143.4ms\n",
      "Speed: 4.0ms preprocess, 143.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Distracted (Confidence: 0.99)\n",
      "\n",
      "0: 480x640 1 person, 149.4ms\n",
      "Speed: 2.0ms preprocess, 149.4ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 146.4ms\n",
      "Speed: 3.0ms preprocess, 146.4ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 150.9ms\n",
      "Speed: 3.0ms preprocess, 150.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 151.9ms\n",
      "Speed: 2.0ms preprocess, 151.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 149.9ms\n",
      "Speed: 4.5ms preprocess, 149.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 144.4ms\n",
      "Speed: 3.0ms preprocess, 144.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 146.4ms\n",
      "Speed: 3.0ms preprocess, 146.4ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 1 remote, 143.8ms\n",
      "Speed: 2.5ms preprocess, 143.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 keyboard, 147.5ms\n",
      "Speed: 3.5ms preprocess, 147.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 2 persons, 1 laptop, 148.3ms\n",
      "Speed: 3.0ms preprocess, 148.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 2 persons, 151.3ms\n",
      "Speed: 2.6ms preprocess, 151.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 146.6ms\n",
      "Speed: 3.0ms preprocess, 146.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 150.9ms\n",
      "Speed: 2.1ms preprocess, 150.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 150.5ms\n",
      "Speed: 3.0ms preprocess, 150.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 1 keyboard, 145.3ms\n",
      "Speed: 4.0ms preprocess, 145.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Distracted (Confidence: 0.96)\n",
      "\n",
      "0: 480x640 1 person, 144.4ms\n",
      "Speed: 3.0ms preprocess, 144.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Distracted (Confidence: 0.97)\n",
      "\n",
      "0: 480x640 1 person, 147.4ms\n",
      "Speed: 2.5ms preprocess, 147.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 143.9ms\n",
      "Speed: 2.0ms preprocess, 143.9ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 145.4ms\n",
      "Speed: 4.5ms preprocess, 145.4ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 146.4ms\n",
      "Speed: 3.5ms preprocess, 146.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 158.4ms\n",
      "Speed: 2.0ms preprocess, 158.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 147.9ms\n",
      "Speed: 3.0ms preprocess, 147.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 146.5ms\n",
      "Speed: 2.0ms preprocess, 146.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 148.4ms\n",
      "Speed: 3.0ms preprocess, 148.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 153.5ms\n",
      "Speed: 4.6ms preprocess, 153.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 142.5ms\n",
      "Speed: 3.0ms preprocess, 142.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 144.9ms\n",
      "Speed: 3.0ms preprocess, 144.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 148.9ms\n",
      "Speed: 3.0ms preprocess, 148.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 149.9ms\n",
      "Speed: 3.0ms preprocess, 149.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 1 keyboard, 152.9ms\n",
      "Speed: 3.5ms preprocess, 152.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 150.6ms\n",
      "Speed: 2.0ms preprocess, 150.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 150.0ms\n",
      "Speed: 3.0ms preprocess, 150.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 144.9ms\n",
      "Speed: 4.5ms preprocess, 144.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 144.3ms\n",
      "Speed: 3.0ms preprocess, 144.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 143.3ms\n",
      "Speed: 3.6ms preprocess, 143.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 148.4ms\n",
      "Speed: 3.0ms preprocess, 148.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 149.5ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed: 2.5ms preprocess, 149.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 148.7ms\n",
      "Speed: 3.0ms preprocess, 148.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 149.9ms\n",
      "Speed: 2.0ms preprocess, 149.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 152.9ms\n",
      "Speed: 3.5ms preprocess, 152.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 146.4ms\n",
      "Speed: 4.5ms preprocess, 146.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 148.3ms\n",
      "Speed: 2.6ms preprocess, 148.3ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 143.9ms\n",
      "Speed: 2.5ms preprocess, 143.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 145.5ms\n",
      "Speed: 2.5ms preprocess, 145.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 144.4ms\n",
      "Speed: 3.5ms preprocess, 144.4ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 148.0ms\n",
      "Speed: 2.0ms preprocess, 148.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 143.9ms\n",
      "Speed: 3.0ms preprocess, 143.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 151.9ms\n",
      "Speed: 3.5ms preprocess, 151.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 149.9ms\n",
      "Speed: 2.5ms preprocess, 149.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 149.9ms\n",
      "Speed: 4.0ms preprocess, 149.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 148.1ms\n",
      "Speed: 3.0ms preprocess, 148.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 151.4ms\n",
      "Speed: 3.0ms preprocess, 151.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 144.8ms\n",
      "Speed: 3.0ms preprocess, 144.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 147.4ms\n",
      "Speed: 3.0ms preprocess, 147.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 1 keyboard, 148.9ms\n",
      "Speed: 3.0ms preprocess, 148.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 149.0ms\n",
      "Speed: 3.0ms preprocess, 149.0ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 156.5ms\n",
      "Speed: 3.5ms preprocess, 156.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 156.3ms\n",
      "Speed: 2.0ms preprocess, 156.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 173.1ms\n",
      "Speed: 3.5ms preprocess, 173.1ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 170.5ms\n",
      "Speed: 3.0ms preprocess, 170.5ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 160.4ms\n",
      "Speed: 3.0ms preprocess, 160.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 159.2ms\n",
      "Speed: 3.0ms preprocess, 159.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 291.3ms\n",
      "Speed: 3.0ms preprocess, 291.3ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 1 keyboard, 168.0ms\n",
      "Speed: 3.0ms preprocess, 168.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 156.9ms\n",
      "Speed: 4.0ms preprocess, 156.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 158.0ms\n",
      "Speed: 3.0ms preprocess, 158.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 1 keyboard, 159.0ms\n",
      "Speed: 4.0ms preprocess, 159.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 154.9ms\n",
      "Speed: 3.0ms preprocess, 154.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 158.4ms\n",
      "Speed: 3.0ms preprocess, 158.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 154.0ms\n",
      "Speed: 4.0ms preprocess, 154.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 166.5ms\n",
      "Speed: 3.5ms preprocess, 166.5ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 158.3ms\n",
      "Speed: 3.6ms preprocess, 158.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 142.9ms\n",
      "Speed: 2.5ms preprocess, 142.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 164.0ms\n",
      "Speed: 3.0ms preprocess, 164.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 1 keyboard, 161.3ms\n",
      "Speed: 3.6ms preprocess, 161.3ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 162.9ms\n",
      "Speed: 3.0ms preprocess, 162.9ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 153.9ms\n",
      "Speed: 3.0ms preprocess, 153.9ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 153.4ms\n",
      "Speed: 2.0ms preprocess, 153.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 1 keyboard, 167.5ms\n",
      "Speed: 3.0ms preprocess, 167.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 166.5ms\n",
      "Speed: 3.5ms preprocess, 166.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 163.8ms\n",
      "Speed: 3.5ms preprocess, 163.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 156.9ms\n",
      "Speed: 2.5ms preprocess, 156.9ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 1 keyboard, 161.9ms\n",
      "Speed: 3.6ms preprocess, 161.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 1 keyboard, 161.1ms\n",
      "Speed: 3.0ms preprocess, 161.1ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 160.5ms\n",
      "Speed: 3.0ms preprocess, 160.5ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 171.9ms\n",
      "Speed: 4.0ms preprocess, 171.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 1 keyboard, 163.8ms\n",
      "Speed: 6.5ms preprocess, 163.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 160.0ms\n",
      "Speed: 3.0ms preprocess, 160.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 153.3ms\n",
      "Speed: 2.5ms preprocess, 153.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 146.8ms\n",
      "Speed: 2.6ms preprocess, 146.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 161.6ms\n",
      "Speed: 2.5ms preprocess, 161.6ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 154.4ms\n",
      "Speed: 4.5ms preprocess, 154.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 146.9ms\n",
      "Speed: 3.0ms preprocess, 146.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Focused (Confidence: 0.01)\n",
      "\n",
      "0: 480x640 1 person, 147.9ms\n",
      "Speed: 4.0ms preprocess, 147.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 149.7ms\n",
      "Speed: 3.5ms preprocess, 149.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 170.0ms\n",
      "Speed: 3.3ms preprocess, 170.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 164.0ms\n",
      "Speed: 3.0ms preprocess, 164.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 156.9ms\n",
      "Speed: 3.0ms preprocess, 156.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 156.0ms\n",
      "Speed: 3.5ms preprocess, 156.0ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 142.8ms\n",
      "Speed: 3.0ms preprocess, 142.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 147.4ms\n",
      "Speed: 3.0ms preprocess, 147.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 1 keyboard, 148.5ms\n",
      "Speed: 3.0ms preprocess, 148.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 144.8ms\n",
      "Speed: 3.6ms preprocess, 144.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 1 keyboard, 150.9ms\n",
      "Speed: 3.0ms preprocess, 150.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 150.4ms\n",
      "Speed: 2.0ms preprocess, 150.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 148.4ms\n",
      "Speed: 2.5ms preprocess, 148.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 147.0ms\n",
      "Speed: 3.5ms preprocess, 147.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 148.4ms\n",
      "Speed: 2.0ms preprocess, 148.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 146.5ms\n",
      "Speed: 3.0ms preprocess, 146.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 143.9ms\n",
      "Speed: 3.0ms preprocess, 143.9ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 148.4ms\n",
      "Speed: 3.0ms preprocess, 148.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 146.4ms\n",
      "Speed: 3.0ms preprocess, 146.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 1 laptop, 148.4ms\n",
      "Speed: 3.5ms preprocess, 148.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 150.9ms\n",
      "Speed: 2.0ms preprocess, 150.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 147.9ms\n",
      "Speed: 3.0ms preprocess, 147.9ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 149.6ms\n",
      "Speed: 3.0ms preprocess, 149.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 153.9ms\n",
      "Speed: 3.0ms preprocess, 153.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 146.9ms\n",
      "Speed: 3.0ms preprocess, 146.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 147.7ms\n",
      "Speed: 4.0ms preprocess, 147.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 1 tie, 145.4ms\n",
      "Speed: 4.5ms preprocess, 145.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 151.4ms\n",
      "Speed: 3.5ms preprocess, 151.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 154.5ms\n",
      "Speed: 3.0ms preprocess, 154.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 151.9ms\n",
      "Speed: 2.0ms preprocess, 151.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 1 tie, 153.9ms\n",
      "Speed: 2.0ms preprocess, 153.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 152.4ms\n",
      "Speed: 3.0ms preprocess, 152.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 1 tie, 159.5ms\n",
      "Speed: 3.5ms preprocess, 159.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 144.8ms\n",
      "Speed: 3.0ms preprocess, 144.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 1 tie, 152.9ms\n",
      "Speed: 3.0ms preprocess, 152.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 153.4ms\n",
      "Speed: 3.0ms preprocess, 153.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 152.9ms\n",
      "Speed: 3.0ms preprocess, 152.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 1 tie, 154.4ms\n",
      "Speed: 3.0ms preprocess, 154.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 157.5ms\n",
      "Speed: 3.6ms preprocess, 157.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 144.3ms\n",
      "Speed: 3.0ms preprocess, 144.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 148.3ms\n",
      "Speed: 2.0ms preprocess, 148.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 148.0ms\n",
      "Speed: 4.2ms preprocess, 148.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 146.9ms\n",
      "Speed: 2.5ms preprocess, 146.9ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 153.3ms\n",
      "Speed: 3.7ms preprocess, 153.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 1 tie, 154.9ms\n",
      "Speed: 3.0ms preprocess, 154.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 1 tie, 154.4ms\n",
      "Speed: 2.5ms preprocess, 154.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 152.4ms\n",
      "Speed: 3.6ms preprocess, 152.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 148.9ms\n",
      "Speed: 3.0ms preprocess, 148.9ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 1 tie, 145.9ms\n",
      "Speed: 2.6ms preprocess, 145.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 145.9ms\n",
      "Speed: 2.0ms preprocess, 145.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 1 tie, 166.5ms\n",
      "Speed: 4.0ms preprocess, 166.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 155.4ms\n",
      "Speed: 3.6ms preprocess, 155.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 151.4ms\n",
      "Speed: 3.5ms preprocess, 151.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 147.9ms\n",
      "Speed: 2.5ms preprocess, 147.9ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 1 tie, 149.4ms\n",
      "Speed: 2.0ms preprocess, 149.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 146.4ms\n",
      "Speed: 2.0ms preprocess, 146.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 143.4ms\n",
      "Speed: 3.0ms preprocess, 143.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 148.3ms\n",
      "Speed: 3.0ms preprocess, 148.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 153.1ms\n",
      "Speed: 3.0ms preprocess, 153.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 149.4ms\n",
      "Speed: 3.0ms preprocess, 149.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 157.4ms\n",
      "Speed: 3.0ms preprocess, 157.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 155.9ms\n",
      "Speed: 3.6ms preprocess, 155.9ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 161.5ms\n",
      "Speed: 2.5ms preprocess, 161.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Focused (Confidence: 0.02)\n",
      "\n",
      "0: 480x640 1 person, 153.3ms\n",
      "Speed: 3.0ms preprocess, 153.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Prediction: Focused (Confidence: 0.05)\n",
      "\n",
      "0: 480x640 1 person, 173.5ms\n",
      "Speed: 3.0ms preprocess, 173.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 188.8ms\n",
      "Speed: 4.0ms preprocess, 188.8ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 153.4ms\n",
      "Speed: 3.0ms preprocess, 153.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 185.5ms\n",
      "Speed: 2.5ms preprocess, 185.5ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 163.6ms\n",
      "Speed: 4.0ms preprocess, 163.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 149.4ms\n",
      "Speed: 3.0ms preprocess, 149.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 156.0ms\n",
      "Speed: 3.0ms preprocess, 156.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 163.5ms\n",
      "Speed: 4.0ms preprocess, 163.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 154.8ms\n",
      "Speed: 3.5ms preprocess, 154.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Prediction: Focused (Confidence: 0.02)\n",
      "\n",
      "0: 480x640 1 person, 156.5ms\n",
      "Speed: 3.0ms preprocess, 156.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Prediction: Focused (Confidence: 0.04)\n",
      "\n",
      "0: 480x640 1 person, 152.5ms\n",
      "Speed: 2.5ms preprocess, 152.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Focused (Confidence: 0.10)\n",
      "\n",
      "0: 480x640 1 person, 149.5ms\n",
      "Speed: 3.0ms preprocess, 149.5ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Prediction: Focused (Confidence: 0.16)\n",
      "\n",
      "0: 480x640 1 person, 144.7ms\n",
      "Speed: 3.0ms preprocess, 144.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Focused (Confidence: 0.06)\n",
      "\n",
      "0: 480x640 1 person, 150.7ms\n",
      "Speed: 3.0ms preprocess, 150.7ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "Prediction: Focused (Confidence: 0.08)\n",
      "\n",
      "0: 480x640 1 person, 1 keyboard, 177.6ms\n",
      "Speed: 4.0ms preprocess, 177.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Prediction: Focused (Confidence: 0.05)\n",
      "\n",
      "0: 480x640 1 person, 160.3ms\n",
      "Speed: 3.0ms preprocess, 160.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Prediction: Focused (Confidence: 0.08)\n",
      "\n",
      "0: 480x640 1 person, 149.9ms\n",
      "Speed: 4.5ms preprocess, 149.9ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Prediction: Focused (Confidence: 0.11)\n",
      "\n",
      "0: 480x640 1 person, 154.1ms\n",
      "Speed: 2.0ms preprocess, 154.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Prediction: Focused (Confidence: 0.13)\n",
      "\n",
      "0: 480x640 1 person, 164.0ms\n",
      "Speed: 2.5ms preprocess, 164.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Prediction: Focused (Confidence: 0.11)\n",
      "\n",
      "0: 480x640 1 person, 152.9ms\n",
      "Speed: 3.0ms preprocess, 152.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Focused (Confidence: 0.19)\n",
      "\n",
      "0: 480x640 1 person, 159.5ms\n",
      "Speed: 4.0ms preprocess, 159.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Focused (Confidence: 0.13)\n",
      "\n",
      "0: 480x640 1 person, 153.9ms\n",
      "Speed: 3.0ms preprocess, 153.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Focused (Confidence: 0.17)\n",
      "\n",
      "0: 480x640 1 person, 158.6ms\n",
      "Speed: 4.0ms preprocess, 158.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Prediction: Focused (Confidence: 0.21)\n",
      "\n",
      "0: 480x640 1 person, 161.0ms\n",
      "Speed: 2.0ms preprocess, 161.0ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Focused (Confidence: 0.16)\n",
      "\n",
      "0: 480x640 1 person, 164.3ms\n",
      "Speed: 3.0ms preprocess, 164.3ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Focused (Confidence: 0.26)\n",
      "\n",
      "0: 480x640 1 person, 153.4ms\n",
      "Speed: 3.0ms preprocess, 153.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Focused (Confidence: 0.14)\n",
      "\n",
      "0: 480x640 1 person, 152.9ms\n",
      "Speed: 2.0ms preprocess, 152.9ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Prediction: Focused (Confidence: 0.20)\n",
      "\n",
      "0: 480x640 1 person, 155.4ms\n",
      "Speed: 3.0ms preprocess, 155.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Prediction: Focused (Confidence: 0.17)\n",
      "\n",
      "0: 480x640 1 person, 150.6ms\n",
      "Speed: 3.5ms preprocess, 150.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Focused (Confidence: 0.19)\n",
      "\n",
      "0: 480x640 1 person, 158.6ms\n",
      "Speed: 3.5ms preprocess, 158.6ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 152.4ms\n",
      "Speed: 3.5ms preprocess, 152.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 150.5ms\n",
      "Speed: 4.5ms preprocess, 150.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 1 keyboard, 155.9ms\n",
      "Speed: 3.0ms preprocess, 155.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 1 keyboard, 156.7ms\n",
      "Speed: 2.0ms preprocess, 156.7ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 1 keyboard, 174.6ms\n",
      "Speed: 3.0ms preprocess, 174.6ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 156.1ms\n",
      "Speed: 3.0ms preprocess, 156.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Focused (Confidence: 0.02)\n",
      "\n",
      "0: 480x640 1 person, 153.4ms\n",
      "Speed: 3.5ms preprocess, 153.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Focused (Confidence: 0.03)\n",
      "\n",
      "0: 480x640 1 person, 174.5ms\n",
      "Speed: 3.0ms preprocess, 174.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "Prediction: Focused (Confidence: 0.10)\n",
      "\n",
      "0: 480x640 1 person, 167.0ms\n",
      "Speed: 3.0ms preprocess, 167.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Prediction: Focused (Confidence: 0.03)\n",
      "\n",
      "0: 480x640 1 person, 149.9ms\n",
      "Speed: 3.0ms preprocess, 149.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Prediction: Focused (Confidence: 0.06)\n",
      "\n",
      "0: 480x640 1 person, 152.4ms\n",
      "Speed: 2.0ms preprocess, 152.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Focused (Confidence: 0.03)\n",
      "\n",
      "0: 480x640 1 person, 148.4ms\n",
      "Speed: 2.5ms preprocess, 148.4ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Focused (Confidence: 0.06)\n",
      "\n",
      "0: 480x640 1 person, 148.7ms\n",
      "Speed: 3.0ms preprocess, 148.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Focused (Confidence: 0.04)\n",
      "\n",
      "0: 480x640 1 person, 142.9ms\n",
      "Speed: 3.5ms preprocess, 142.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Prediction: Focused (Confidence: 0.04)\n",
      "\n",
      "0: 480x640 1 person, 145.2ms\n",
      "Speed: 3.0ms preprocess, 145.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Focused (Confidence: 0.05)\n",
      "\n",
      "0: 480x640 1 person, 148.3ms\n",
      "Speed: 2.6ms preprocess, 148.3ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Focused (Confidence: 0.08)\n",
      "\n",
      "0: 480x640 1 person, 143.8ms\n",
      "Speed: 2.6ms preprocess, 143.8ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Prediction: Focused (Confidence: 0.13)\n",
      "\n",
      "0: 480x640 1 person, 164.4ms\n",
      "Speed: 4.5ms preprocess, 164.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Prediction: Focused (Confidence: 0.18)\n",
      "\n",
      "0: 480x640 1 person, 142.8ms\n",
      "Speed: 2.0ms preprocess, 142.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Focused (Confidence: 0.10)\n",
      "\n",
      "0: 480x640 1 person, 143.9ms\n",
      "Speed: 2.5ms preprocess, 143.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Prediction: Focused (Confidence: 0.13)\n",
      "\n",
      "0: 480x640 1 person, 147.0ms\n",
      "Speed: 2.0ms preprocess, 147.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Prediction: Focused (Confidence: 0.11)\n",
      "\n",
      "0: 480x640 1 person, 142.4ms\n",
      "Speed: 3.0ms preprocess, 142.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Prediction: Focused (Confidence: 0.10)\n",
      "\n",
      "0: 480x640 1 person, 141.9ms\n",
      "Speed: 3.5ms preprocess, 141.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Focused (Confidence: 0.10)\n",
      "\n",
      "0: 480x640 1 person, 139.9ms\n",
      "Speed: 3.0ms preprocess, 139.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Prediction: Focused (Confidence: 0.16)\n",
      "\n",
      "0: 480x640 1 person, 142.0ms\n",
      "Speed: 2.5ms preprocess, 142.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Prediction: Focused (Confidence: 0.18)\n",
      "\n",
      "0: 480x640 1 person, 140.4ms\n",
      "Speed: 3.0ms preprocess, 140.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Focused (Confidence: 0.22)\n",
      "\n",
      "0: 480x640 1 person, 139.4ms\n",
      "Speed: 3.0ms preprocess, 139.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Focused (Confidence: 0.18)\n",
      "\n",
      "0: 480x640 1 person, 1 keyboard, 143.4ms\n",
      "Speed: 3.0ms preprocess, 143.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Prediction: Focused (Confidence: 0.07)\n",
      "\n",
      "0: 480x640 1 person, 143.6ms\n",
      "Speed: 2.0ms preprocess, 143.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Focused (Confidence: 0.10)\n",
      "\n",
      "0: 480x640 1 person, 1 keyboard, 139.4ms\n",
      "Speed: 2.5ms preprocess, 139.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Prediction: Focused (Confidence: 0.10)\n",
      "\n",
      "0: 480x640 1 person, 1 keyboard, 146.8ms\n",
      "Speed: 3.1ms preprocess, 146.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Focused (Confidence: 0.10)\n",
      "\n",
      "0: 480x640 1 person, 142.4ms\n",
      "Speed: 3.0ms preprocess, 142.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Focused (Confidence: 0.13)\n",
      "\n",
      "0: 480x640 1 person, 1 keyboard, 143.4ms\n",
      "Speed: 2.0ms preprocess, 143.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Focused (Confidence: 0.16)\n",
      "\n",
      "0: 480x640 1 person, 140.3ms\n",
      "Speed: 3.6ms preprocess, 140.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Focused (Confidence: 0.24)\n",
      "\n",
      "0: 480x640 1 person, 137.4ms\n",
      "Speed: 3.0ms preprocess, 137.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Focused (Confidence: 0.20)\n",
      "\n",
      "0: 480x640 1 person, 141.0ms\n",
      "Speed: 2.5ms preprocess, 141.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Prediction: Focused (Confidence: 0.16)\n",
      "\n",
      "0: 480x640 1 person, 139.9ms\n",
      "Speed: 2.5ms preprocess, 139.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Prediction: Focused (Confidence: 0.18)\n",
      "\n",
      "0: 480x640 1 person, 139.9ms\n",
      "Speed: 2.0ms preprocess, 139.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Focused (Confidence: 0.16)\n",
      "\n",
      "0: 480x640 1 person, 152.4ms\n",
      "Speed: 3.0ms preprocess, 152.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Focused (Confidence: 0.10)\n",
      "\n",
      "0: 480x640 1 person, 144.8ms\n",
      "Speed: 2.5ms preprocess, 144.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Prediction: Focused (Confidence: 0.14)\n",
      "\n",
      "0: 480x640 1 person, 142.3ms\n",
      "Speed: 3.0ms preprocess, 142.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Focused (Confidence: 0.11)\n",
      "\n",
      "0: 480x640 1 person, 140.1ms\n",
      "Speed: 3.0ms preprocess, 140.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Prediction: Focused (Confidence: 0.12)\n",
      "\n",
      "0: 480x640 1 person, 145.8ms\n",
      "Speed: 2.0ms preprocess, 145.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Focused (Confidence: 0.10)\n",
      "\n",
      "0: 480x640 1 person, 152.0ms\n",
      "Speed: 3.0ms preprocess, 152.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Prediction: Focused (Confidence: 0.07)\n",
      "\n",
      "0: 480x640 1 person, 138.5ms\n",
      "Speed: 3.0ms preprocess, 138.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Focused (Confidence: 0.09)\n",
      "\n",
      "0: 480x640 1 person, 136.4ms\n",
      "Speed: 2.0ms preprocess, 136.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Focused (Confidence: 0.13)\n",
      "\n",
      "0: 480x640 1 person, 137.9ms\n",
      "Speed: 3.0ms preprocess, 137.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Prediction: Focused (Confidence: 0.14)\n",
      "\n",
      "0: 480x640 1 person, 137.9ms\n",
      "Speed: 2.5ms preprocess, 137.9ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Prediction: Focused (Confidence: 0.12)\n",
      "\n",
      "0: 480x640 1 person, 144.4ms\n",
      "Speed: 2.0ms preprocess, 144.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Focused (Confidence: 0.15)\n",
      "\n",
      "0: 480x640 1 person, 142.9ms\n",
      "Speed: 2.0ms preprocess, 142.9ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Focused (Confidence: 0.11)\n",
      "\n",
      "0: 480x640 1 person, 154.0ms\n",
      "Speed: 3.5ms preprocess, 154.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Prediction: Focused (Confidence: 0.12)\n",
      "\n",
      "0: 480x640 1 person, 141.8ms\n",
      "Speed: 2.0ms preprocess, 141.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Focused (Confidence: 0.14)\n",
      "\n",
      "0: 480x640 1 person, 151.4ms\n",
      "Speed: 2.0ms preprocess, 151.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Focused (Confidence: 0.11)\n",
      "\n",
      "0: 480x640 1 person, 140.4ms\n",
      "Speed: 2.0ms preprocess, 140.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Focused (Confidence: 0.12)\n",
      "\n",
      "0: 480x640 1 person, 138.9ms\n",
      "Speed: 3.5ms preprocess, 138.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Focused (Confidence: 0.12)\n",
      "\n",
      "0: 480x640 1 person, 135.8ms\n",
      "Speed: 3.0ms preprocess, 135.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Focused (Confidence: 0.07)\n",
      "\n",
      "0: 480x640 1 person, 142.9ms\n",
      "Speed: 3.0ms preprocess, 142.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Focused (Confidence: 0.06)\n",
      "\n",
      "0: 480x640 1 person, 140.9ms\n",
      "Speed: 1.5ms preprocess, 140.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Focused (Confidence: 0.15)\n",
      "\n",
      "0: 480x640 1 person, 143.4ms\n",
      "Speed: 2.0ms preprocess, 143.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Focused (Confidence: 0.08)\n",
      "\n",
      "0: 480x640 1 person, 144.9ms\n",
      "Speed: 2.0ms preprocess, 144.9ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Focused (Confidence: 0.07)\n",
      "\n",
      "0: 480x640 1 person, 140.4ms\n",
      "Speed: 2.5ms preprocess, 140.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Focused (Confidence: 0.06)\n",
      "\n",
      "0: 480x640 1 person, 144.9ms\n",
      "Speed: 3.0ms preprocess, 144.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Focused (Confidence: 0.08)\n",
      "\n",
      "0: 480x640 1 person, 143.5ms\n",
      "Speed: 4.0ms preprocess, 143.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Focused (Confidence: 0.14)\n",
      "\n",
      "0: 480x640 1 person, 137.9ms\n",
      "Speed: 2.0ms preprocess, 137.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Prediction: Focused (Confidence: 0.06)\n",
      "\n",
      "0: 480x640 1 person, 137.3ms\n",
      "Speed: 3.0ms preprocess, 137.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Focused (Confidence: 0.11)\n",
      "\n",
      "0: 480x640 1 person, 139.8ms\n",
      "Speed: 2.0ms preprocess, 139.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Focused (Confidence: 0.08)\n",
      "\n",
      "0: 480x640 1 person, 136.7ms\n",
      "Speed: 3.0ms preprocess, 136.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Prediction: Focused (Confidence: 0.05)\n",
      "\n",
      "0: 480x640 1 person, 140.6ms\n",
      "Speed: 2.0ms preprocess, 140.6ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Focused (Confidence: 0.07)\n",
      "\n",
      "0: 480x640 1 person, 140.9ms\n",
      "Speed: 2.0ms preprocess, 140.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Prediction: Focused (Confidence: 0.06)\n",
      "\n",
      "0: 480x640 1 person, 150.4ms\n",
      "Speed: 2.0ms preprocess, 150.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Focused (Confidence: 0.06)\n",
      "\n",
      "0: 480x640 1 person, 137.8ms\n",
      "Speed: 2.0ms preprocess, 137.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Prediction: Focused (Confidence: 0.04)\n",
      "\n",
      "0: 480x640 1 person, 142.0ms\n",
      "Speed: 2.0ms preprocess, 142.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Prediction: Focused (Confidence: 0.04)\n",
      "\n",
      "0: 480x640 1 person, 138.4ms\n",
      "Speed: 3.0ms preprocess, 138.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Focused (Confidence: 0.09)\n",
      "\n",
      "0: 480x640 1 person, 156.0ms\n",
      "Speed: 2.0ms preprocess, 156.0ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Focused (Confidence: 0.06)\n",
      "\n",
      "0: 480x640 1 person, 140.1ms\n",
      "Speed: 2.0ms preprocess, 140.1ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Focused (Confidence: 0.06)\n",
      "\n",
      "0: 480x640 1 person, 140.2ms\n",
      "Speed: 3.0ms preprocess, 140.2ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Focused (Confidence: 0.23)\n",
      "\n",
      "0: 480x640 1 person, 145.9ms\n",
      "Speed: 3.0ms preprocess, 145.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Focused (Confidence: 0.33)\n",
      "\n",
      "0: 480x640 1 person, 138.5ms\n",
      "Speed: 2.0ms preprocess, 138.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Focused (Confidence: 0.35)\n",
      "\n",
      "0: 480x640 1 person, 142.0ms\n",
      "Speed: 2.5ms preprocess, 142.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Prediction: Focused (Confidence: 0.29)\n",
      "\n",
      "0: 480x640 1 person, 142.6ms\n",
      "Speed: 2.0ms preprocess, 142.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Prediction: Focused (Confidence: 0.34)\n",
      "\n",
      "0: 480x640 1 person, 264.8ms\n",
      "Speed: 2.0ms preprocess, 264.8ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "Prediction: Focused (Confidence: 0.26)\n",
      "\n",
      "0: 480x640 1 person, 148.4ms\n",
      "Speed: 2.0ms preprocess, 148.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Prediction: Focused (Confidence: 0.41)\n",
      "\n",
      "0: 480x640 1 person, 147.8ms\n",
      "Speed: 2.0ms preprocess, 147.8ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Prediction: Distracted (Confidence: 0.50)\n",
      "\n",
      "0: 480x640 1 person, 152.0ms\n",
      "Speed: 3.0ms preprocess, 152.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Distracted (Confidence: 0.59)\n",
      "\n",
      "0: 480x640 1 person, 159.5ms\n",
      "Speed: 3.5ms preprocess, 159.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Focused (Confidence: 0.43)\n",
      "\n",
      "0: 480x640 1 person, 159.5ms\n",
      "Speed: 2.0ms preprocess, 159.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Prediction: Focused (Confidence: 0.16)\n",
      "\n",
      "0: 480x640 1 person, 156.6ms\n",
      "Speed: 2.5ms preprocess, 156.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Focused (Confidence: 0.12)\n",
      "\n",
      "0: 480x640 1 person, 157.6ms\n",
      "Speed: 3.0ms preprocess, 157.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Prediction: Focused (Confidence: 0.10)\n",
      "\n",
      "0: 480x640 1 person, 151.9ms\n",
      "Speed: 3.0ms preprocess, 151.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Prediction: Focused (Confidence: 0.13)\n",
      "\n",
      "0: 480x640 1 person, 163.5ms\n",
      "Speed: 4.6ms preprocess, 163.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Prediction: Focused (Confidence: 0.03)\n",
      "\n",
      "0: 480x640 1 person, 159.2ms\n",
      "Speed: 3.0ms preprocess, 159.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Prediction: Focused (Confidence: 0.03)\n",
      "\n",
      "0: 480x640 1 person, 146.9ms\n",
      "Speed: 3.1ms preprocess, 146.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Focused (Confidence: 0.04)\n",
      "\n",
      "0: 480x640 1 person, 148.5ms\n",
      "Speed: 4.0ms preprocess, 148.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Focused (Confidence: 0.11)\n",
      "\n",
      "0: 480x640 1 person, 158.6ms\n",
      "Speed: 3.0ms preprocess, 158.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Focused (Confidence: 0.10)\n",
      "\n",
      "0: 480x640 1 person, 157.4ms\n",
      "Speed: 4.0ms preprocess, 157.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Prediction: Focused (Confidence: 0.09)\n",
      "\n",
      "0: 480x640 1 person, 156.5ms\n",
      "Speed: 3.0ms preprocess, 156.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Focused (Confidence: 0.19)\n",
      "\n",
      "0: 480x640 1 person, 156.5ms\n",
      "Speed: 3.0ms preprocess, 156.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Focused (Confidence: 0.21)\n",
      "\n",
      "0: 480x640 1 person, 155.5ms\n",
      "Speed: 3.0ms preprocess, 155.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Focused (Confidence: 0.23)\n",
      "\n",
      "0: 480x640 1 person, 162.1ms\n",
      "Speed: 3.0ms preprocess, 162.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Focused (Confidence: 0.12)\n",
      "\n",
      "0: 480x640 1 person, 158.5ms\n",
      "Speed: 3.0ms preprocess, 158.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Focused (Confidence: 0.17)\n",
      "\n",
      "0: 480x640 1 person, 151.4ms\n",
      "Speed: 3.0ms preprocess, 151.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Focused (Confidence: 0.12)\n",
      "\n",
      "0: 480x640 1 person, 149.1ms\n",
      "Speed: 3.0ms preprocess, 149.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Prediction: Focused (Confidence: 0.19)\n",
      "\n",
      "0: 480x640 1 person, 154.5ms\n",
      "Speed: 2.0ms preprocess, 154.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Focused (Confidence: 0.21)\n",
      "\n",
      "0: 480x640 1 person, 146.8ms\n",
      "Speed: 4.0ms preprocess, 146.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Focused (Confidence: 0.33)\n",
      "\n",
      "0: 480x640 1 person, 150.4ms\n",
      "Speed: 3.5ms preprocess, 150.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Focused (Confidence: 0.11)\n",
      "\n",
      "0: 480x640 1 person, 152.9ms\n",
      "Speed: 3.0ms preprocess, 152.9ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Focused (Confidence: 0.08)\n",
      "\n",
      "0: 480x640 1 person, 166.1ms\n",
      "Speed: 3.0ms preprocess, 166.1ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Prediction: Focused (Confidence: 0.16)\n",
      "\n",
      "0: 480x640 1 person, 148.9ms\n",
      "Speed: 3.0ms preprocess, 148.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Focused (Confidence: 0.16)\n",
      "\n",
      "0: 480x640 1 person, 149.4ms\n",
      "Speed: 3.0ms preprocess, 149.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Focused (Confidence: 0.08)\n",
      "\n",
      "0: 480x640 1 person, 155.4ms\n",
      "Speed: 2.5ms preprocess, 155.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Focused (Confidence: 0.06)\n",
      "\n",
      "0: 480x640 1 person, 147.3ms\n",
      "Speed: 4.0ms preprocess, 147.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Focused (Confidence: 0.07)\n",
      "\n",
      "0: 480x640 1 person, 142.4ms\n",
      "Speed: 3.0ms preprocess, 142.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Prediction: Focused (Confidence: 0.07)\n",
      "\n",
      "0: 480x640 1 person, 141.8ms\n",
      "Speed: 3.0ms preprocess, 141.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Focused (Confidence: 0.10)\n",
      "\n",
      "0: 480x640 1 person, 145.9ms\n",
      "Speed: 3.5ms preprocess, 145.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
      "Prediction: Focused (Confidence: 0.08)\n",
      "\n",
      "0: 480x640 1 person, 150.4ms\n",
      "Speed: 7.6ms preprocess, 150.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Focused (Confidence: 0.09)\n",
      "\n",
      "0: 480x640 1 person, 148.4ms\n",
      "Speed: 3.5ms preprocess, 148.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Focused (Confidence: 0.12)\n",
      "\n",
      "0: 480x640 1 person, 146.9ms\n",
      "Speed: 3.0ms preprocess, 146.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Focused (Confidence: 0.15)\n",
      "\n",
      "0: 480x640 1 person, 147.4ms\n",
      "Speed: 2.5ms preprocess, 147.4ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Focused (Confidence: 0.13)\n",
      "\n",
      "0: 480x640 1 person, 147.0ms\n",
      "Speed: 3.0ms preprocess, 147.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Prediction: Focused (Confidence: 0.12)\n",
      "\n",
      "0: 480x640 1 person, 146.4ms\n",
      "Speed: 3.0ms preprocess, 146.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Focused (Confidence: 0.09)\n",
      "\n",
      "0: 480x640 1 person, 150.9ms\n",
      "Speed: 3.0ms preprocess, 150.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Focused (Confidence: 0.11)\n",
      "\n",
      "0: 480x640 1 person, 151.6ms\n",
      "Speed: 2.5ms preprocess, 151.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Focused (Confidence: 0.11)\n",
      "\n",
      "0: 480x640 1 person, 147.9ms\n",
      "Speed: 4.0ms preprocess, 147.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Prediction: Focused (Confidence: 0.09)\n",
      "\n",
      "0: 480x640 1 person, 149.4ms\n",
      "Speed: 3.0ms preprocess, 149.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Focused (Confidence: 0.08)\n",
      "\n",
      "0: 480x640 1 person, 152.0ms\n",
      "Speed: 2.5ms preprocess, 152.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Prediction: Focused (Confidence: 0.02)\n",
      "\n",
      "0: 480x640 1 person, 145.9ms\n",
      "Speed: 2.5ms preprocess, 145.9ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Focused (Confidence: 0.15)\n",
      "\n",
      "0: 480x640 1 person, 141.9ms\n",
      "Speed: 3.6ms preprocess, 141.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Focused (Confidence: 0.05)\n",
      "\n",
      "0: 480x640 1 person, 142.4ms\n",
      "Speed: 2.0ms preprocess, 142.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Focused (Confidence: 0.11)\n",
      "\n",
      "0: 480x640 1 person, 144.9ms\n",
      "Speed: 3.0ms preprocess, 144.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Focused (Confidence: 0.11)\n",
      "\n",
      "0: 480x640 1 person, 147.8ms\n",
      "Speed: 3.0ms preprocess, 147.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Focused (Confidence: 0.13)\n",
      "\n",
      "0: 480x640 1 person, 162.5ms\n",
      "Speed: 3.5ms preprocess, 162.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Prediction: Focused (Confidence: 0.15)\n",
      "\n",
      "0: 480x640 1 person, 155.4ms\n",
      "Speed: 3.0ms preprocess, 155.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Focused (Confidence: 0.12)\n",
      "\n",
      "0: 480x640 1 person, 150.0ms\n",
      "Speed: 2.5ms preprocess, 150.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Focused (Confidence: 0.14)\n",
      "\n",
      "0: 480x640 1 person, 148.1ms\n",
      "Speed: 3.5ms preprocess, 148.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Focused (Confidence: 0.12)\n",
      "\n",
      "0: 480x640 1 person, 147.9ms\n",
      "Speed: 3.5ms preprocess, 147.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Focused (Confidence: 0.21)\n",
      "\n",
      "0: 480x640 1 person, 160.6ms\n",
      "Speed: 3.0ms preprocess, 160.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Focused (Confidence: 0.10)\n",
      "\n",
      "0: 480x640 1 person, 157.9ms\n",
      "Speed: 3.5ms preprocess, 157.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Focused (Confidence: 0.14)\n",
      "\n",
      "0: 480x640 1 person, 151.9ms\n",
      "Speed: 3.5ms preprocess, 151.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Focused (Confidence: 0.17)\n",
      "\n",
      "0: 480x640 1 person, 155.5ms\n",
      "Speed: 4.0ms preprocess, 155.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Focused (Confidence: 0.14)\n",
      "\n",
      "0: 480x640 1 person, 154.9ms\n",
      "Speed: 3.6ms preprocess, 154.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Prediction: Focused (Confidence: 0.11)\n",
      "\n",
      "0: 480x640 1 person, 155.9ms\n",
      "Speed: 2.0ms preprocess, 155.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Focused (Confidence: 0.19)\n",
      "\n",
      "0: 480x640 1 person, 152.4ms\n",
      "Speed: 3.0ms preprocess, 152.4ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Focused (Confidence: 0.14)\n",
      "\n",
      "0: 480x640 1 person, 161.5ms\n",
      "Speed: 3.1ms preprocess, 161.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Focused (Confidence: 0.06)\n",
      "\n",
      "0: 480x640 1 person, 149.4ms\n",
      "Speed: 3.0ms preprocess, 149.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 156.5ms\n",
      "Speed: 3.0ms preprocess, 156.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Prediction: Focused (Confidence: 0.00)\n",
      "\n",
      "0: 480x640 1 person, 148.4ms\n",
      "Speed: 3.0ms preprocess, 148.4ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Prediction: Focused (Confidence: 0.10)\n",
      "\n",
      "0: 480x640 1 person, 154.4ms\n",
      "Speed: 3.5ms preprocess, 154.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Focused (Confidence: 0.30)\n",
      "\n",
      "0: 480x640 1 person, 155.9ms\n",
      "Speed: 3.0ms preprocess, 155.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Focused (Confidence: 0.24)\n",
      "\n",
      "0: 480x640 1 person, 154.0ms\n",
      "Speed: 3.0ms preprocess, 154.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Prediction: Focused (Confidence: 0.28)\n",
      "\n",
      "0: 480x640 1 person, 156.5ms\n",
      "Speed: 4.0ms preprocess, 156.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Distracted (Confidence: 0.73)\n",
      "\n",
      "0: 480x640 1 person, 154.9ms\n",
      "Speed: 3.0ms preprocess, 154.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Distracted (Confidence: 0.70)\n",
      "\n",
      "0: 480x640 1 person, 146.4ms\n",
      "Speed: 2.0ms preprocess, 146.4ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Distracted (Confidence: 0.50)\n",
      "\n",
      "0: 480x640 1 person, 154.9ms\n",
      "Speed: 2.6ms preprocess, 154.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Distracted (Confidence: 0.51)\n",
      "\n",
      "0: 480x640 1 person, 156.4ms\n",
      "Speed: 3.0ms preprocess, 156.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Distracted (Confidence: 0.83)\n",
      "\n",
      "0: 480x640 1 person, 153.9ms\n",
      "Speed: 4.0ms preprocess, 153.9ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Distracted (Confidence: 0.98)\n",
      "\n",
      "0: 480x640 1 person, 151.4ms\n",
      "Speed: 3.0ms preprocess, 151.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 152.9ms\n",
      "Speed: 3.0ms preprocess, 152.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 152.7ms\n",
      "Speed: 4.5ms preprocess, 152.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 1 laptop, 148.8ms\n",
      "Speed: 3.0ms preprocess, 148.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 149.9ms\n",
      "Speed: 3.0ms preprocess, 149.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 158.9ms\n",
      "Speed: 3.5ms preprocess, 158.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 158.4ms\n",
      "Speed: 3.0ms preprocess, 158.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 156.4ms\n",
      "Speed: 3.0ms preprocess, 156.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 152.0ms\n",
      "Speed: 2.0ms preprocess, 152.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 149.5ms\n",
      "Speed: 3.5ms preprocess, 149.5ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 149.9ms\n",
      "Speed: 2.5ms preprocess, 149.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Prediction: Distracted (Confidence: 0.98)\n",
      "\n",
      "0: 480x640 1 person, 149.6ms\n",
      "Speed: 3.0ms preprocess, 149.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Distracted (Confidence: 0.98)\n",
      "\n",
      "0: 480x640 1 person, 147.7ms\n",
      "Speed: 3.0ms preprocess, 147.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 148.0ms\n",
      "Speed: 3.5ms preprocess, 148.0ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 151.5ms\n",
      "Speed: 2.0ms preprocess, 151.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 148.6ms\n",
      "Speed: 3.0ms preprocess, 148.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 149.9ms\n",
      "Speed: 2.5ms preprocess, 149.9ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 147.3ms\n",
      "Speed: 3.0ms preprocess, 147.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 152.4ms\n",
      "Speed: 3.0ms preprocess, 152.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 141.1ms\n",
      "Speed: 3.5ms preprocess, 141.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 146.9ms\n",
      "Speed: 3.5ms preprocess, 146.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 146.8ms\n",
      "Speed: 2.0ms preprocess, 146.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 144.0ms\n",
      "Speed: 3.5ms preprocess, 144.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 146.9ms\n",
      "Speed: 3.0ms preprocess, 146.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 146.4ms\n",
      "Speed: 2.0ms preprocess, 146.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 148.3ms\n",
      "Speed: 3.0ms preprocess, 148.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 144.9ms\n",
      "Speed: 3.5ms preprocess, 144.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 143.4ms\n",
      "Speed: 3.0ms preprocess, 143.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 140.8ms\n",
      "Speed: 3.0ms preprocess, 140.8ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 149.4ms\n",
      "Speed: 3.0ms preprocess, 149.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n",
      "\n",
      "0: 480x640 1 person, 145.4ms\n",
      "Speed: 3.0ms preprocess, 145.4ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Prediction: Distracted (Confidence: 1.00)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    get_class_duration()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d5bc3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3d421f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
